{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50ebd2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import autograd.scipy.stats as sps_autograd\n",
    "from autograd import grad, hessian, jacobian\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import toeplitz\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f7dd28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simulate ARMA(1, 1) model\n",
    "\"\"\"\n",
    "a = 0.5\n",
    "b = 0.2\n",
    "# Define AR and MA coefficients\n",
    "ar = np.array([1, -a])  \n",
    "ma = np.array([1, -b])        \n",
    "\n",
    "# Create ARMA process object\n",
    "arma_process = ArmaProcess(ar, ma)\n",
    "\n",
    "# Simulate 500 samples\n",
    "N = 400\n",
    "y = arma_process.generate_sample(nsample=N, scale=2) # scale is the variance of the white noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "971b2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_KF_parameter_values(theta, sigma2):\n",
    "    \"\"\"\n",
    "    Initialise the parameters for the Kalman Filter\n",
    "    \"\"\"\n",
    "    # region Initial values for Kalman Filter\n",
    "    a, b = theta\n",
    "    k = len(theta)\n",
    "\n",
    "    # Simple operations that autograd can handle\n",
    "    F = np.array([[a, 1.0], [0.0, 0.0]])\n",
    "    G = np.array([[1.0], [-b]])\n",
    "    H = np.array([[1.0, 0.0]])\n",
    "    Q = np.eye(k)\n",
    "    dF = np.array([\n",
    "        [[1, 0], [0, 0]],  # First parameter (AR coefficient)\n",
    "        [[0, 0], [0, 0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dF.reshape(2, 2, 2)\n",
    "    dG = np.array([\n",
    "        [[0], [0]],  # First parameter (AR coefficient)\n",
    "        [[0], [-1.0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dG.reshape(2, 2, 1)\n",
    "\n",
    "    g = np.array([1.0, a - b, a * (a - b)])\n",
    "\n",
    "    C = np.array([\n",
    "        sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2),\n",
    "        a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b,\n",
    "        a * (a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b)\n",
    "    ])\n",
    "\n",
    "    V = np.array([\n",
    "        [C[0], -b * g[0]],\n",
    "        [-b * g[0], b**2 * sigma2]\n",
    "    ])\n",
    "\n",
    "    dV = np.array([\n",
    "        [[(2 * sigma2 * (a-b) * (1 - a*b)) / (1 - a**2)**2, 0],\n",
    "        [0, 0]],\n",
    "        [[2 * sigma2 * (b-a) / (1 - a**2), -1],\n",
    "        [-1, 2 * b * sigma2]]\n",
    "    ])\n",
    "\n",
    "    # Initialize the x and dx\n",
    "    x = np.zeros((k, 1))\n",
    "    dx = np.zeros((2, 2, 1))\n",
    "\n",
    "    return F, G, H, dF, dG, V, dV, x, dx, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac6efedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_grad_hess(theta, init_sigma2):\n",
    "    a, b = theta\n",
    "    k = 2\n",
    "    sigma2 = init_sigma2  # CHANGE PARAMETER HERE\n",
    "\n",
    "    # Simple operations that autograd can handle\n",
    "    F = np.array([[a, 1.0], [0.0, 0.0]])\n",
    "    G = np.array([[1.0], [-b]])\n",
    "    H = np.array([[1.0, 0.0]])\n",
    "    Q = np.eye(k)\n",
    "    dF = np.array([\n",
    "        [[1, 0], [0, 0]],  # First parameter (AR coefficient)\n",
    "        [[0, 0], [0, 0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dF.reshape(2, 2, 2)\n",
    "    dG = np.array([\n",
    "        [[0], [0]],  # First parameter (AR coefficient)\n",
    "        [[0], [-1.0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dG.reshape(2, 2, 1)\n",
    "\n",
    "    g = np.array([1.0, a - b, a * (a - b)])\n",
    "\n",
    "    C = np.array([\n",
    "        sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2),\n",
    "        a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b,\n",
    "        a * (a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b)\n",
    "    ])\n",
    "\n",
    "    V = np.array([\n",
    "        [C[0], -b * g[0]],\n",
    "        [-b * g[0], b**2 * sigma2]\n",
    "    ])\n",
    "\n",
    "    dV = np.array([\n",
    "        [[(2 * sigma2 * (a-b) * (1 - a*b)) / (1 - a**2)**2, 0],\n",
    "        [0, 0]],\n",
    "        [[2 * sigma2 * (b-a) / (1 - a**2), -1],\n",
    "        [-1, 2 * b * sigma2]]\n",
    "    ])\n",
    "\n",
    "    # Initialize the x and dx\n",
    "    x = np.zeros((k, 1))\n",
    "    dx = np.zeros((2, 2, 1))\n",
    "\n",
    "    # Dictionary to store the values\n",
    "    dict = {\n",
    "        'analytical_grad_at_i': [],\n",
    "        'log_likelihood_at_i': [],\n",
    "    }\n",
    "\n",
    "    # Initialize the values\n",
    "    sigma2_hat_sum = 0.0\n",
    "    dsigma2_hat_sum = np.array([[0.0], [0.0]]).reshape(2, 1, 1)\n",
    "    log_r_sum = 0.0\n",
    "\n",
    "\n",
    "    # Run the Kalman filter for gradient\n",
    "    for t in range(N):\n",
    "        # Set the current sample size\n",
    "        n = t + 1\n",
    "        \n",
    "\n",
    "        # 1. Predict\n",
    "        # Predict one-step-ahead state predictive density of x_{t}\n",
    "        x_predict = F @ x\n",
    "        V_predict = F @ V @ F.T + G @ G.T\n",
    "\n",
    "        # Compute forecast error and one-step-ahead predictive variance\n",
    "        e_t = y[t] - (H @ x_predict)[0, 0]\n",
    "        r_t = (H @ V_predict @ H.T)[0, 0]\n",
    "\n",
    "        GdGT = np.array([G @ dG.T[0][i].reshape(1,2) for i in range(2)])\n",
    "\n",
    "        # Kalman filter for gradient\n",
    "        dx_predict = F @ dx + dF @ x\n",
    "        dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + GdGT\n",
    "        # dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + G @ Q @ dG.T\n",
    "\n",
    "\n",
    "        # Calculate de_t and dr_t as tensor(2,1,1)\n",
    "        de_t = -H @ dx_predict\n",
    "        dr_t = H @ dV_predict @ H.T\n",
    "\n",
    "        # Update sigma2 hat and gradient of sigma2 hat\n",
    "        sigma2_hat_sum += e_t**2 / r_t\n",
    "        sigma2_hat = sigma2_hat_sum / n\n",
    "        dsigma2_hat_sum += (2 * e_t * de_t) /r_t - (e_t**2 * dr_t) / (r_t**2)\n",
    "        dsigma2_hat = dsigma2_hat_sum / n\n",
    "\n",
    "        # Update the r\n",
    "        log_r_sum += np.log(r_t)\n",
    "\n",
    "        # 2. Update\n",
    "        # Kalman gain\n",
    "        K = V_predict @ H.T / r_t\n",
    "\n",
    "        # Update current state and covariance\n",
    "        x = x_predict + K * e_t\n",
    "        V = (np.eye(k) - K @ H) @ V_predict\n",
    "\n",
    "        dK = (dV_predict @ H.T / r_t) - (V_predict @ H.T / r_t**2) @ dr_t\n",
    "        dx = dx_predict + K @ de_t + dK * e_t\n",
    "        dV = dV_predict - dK @ H @ V_predict - K @ H @ dV_predict\n",
    "        \n",
    "        # Compute sigma2_hat and gradient of the log-likelihood\n",
    "        log_likelihood_at_i = -0.5 * (np.log(2 * np.pi) \n",
    "                                + np.log(sigma2_hat) \n",
    "                                + np.log(r_t) + e_t**2 / (r_t * sigma2_hat))  \n",
    "        \n",
    "        \"\"\"\n",
    "        analytical_grad = - (e_t * de_t) / (sigma2_hat * r_t) \\\n",
    "                                + (e_t**2 * dr_t) / (2 * sigma2_hat * r_t**2) \\\n",
    "                                - dr_t / (2 * r_t)\n",
    "        \"\"\"\n",
    "        \n",
    "        analytical_grad_at_i = - 0.5 * (dsigma2_hat / sigma2_hat \n",
    "                                    + dr_t / r_t \n",
    "                                    + (2 * e_t * sigma2_hat * r_t * de_t - e_t**2 * r_t * dsigma2_hat - e_t**2 * sigma2_hat * dr_t) / (sigma2_hat**2 * r_t**2)\n",
    "                                    )\n",
    "        \n",
    "\n",
    "        dict['analytical_grad_at_i'].append(analytical_grad_at_i.flatten())\n",
    "        dict['log_likelihood_at_i'].append(log_likelihood_at_i)\n",
    "\n",
    "    log_likelihood = -0.5 * (N * np.log(2 * np.pi) \n",
    "                            + N * np.log(sigma2_hat) \n",
    "                            # + np.sum(np.log(r)) + N)\n",
    "                            + log_r_sum + N)\n",
    "\n",
    "    # Return the dictionary\n",
    "    return dict, sigma2_hat, log_likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6e116bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# Initialize the parameters\\ntheta = np.array([-0.2, 0.1])\\ninit_sigma2 = 0.5\\n\\n# Dataframe to store the results\\ndf = pd.DataFrame(columns=['t', 'analytical_hess', 'auto_hess', 'equal'])\\n\\nfor t in range(N):\\n    # Analytical gradient\\n    obj_func_gradient = lambda param: analytical_grad_hess(param, init_sigma2)[0]['analytical_grad_at_i'][t]\\n    jacobian_obj_func_gradient = jacobian(obj_func_gradient)\\n    auto_jacobian_val = jacobian_obj_func_gradient(theta)\\n\\n    # Auto Hessian\\n    obj_func_likelihood = lambda param: analytical_grad_hess(param, init_sigma2)[0]['log_likelihood_at_i'][t]\\n    hess_obj_func_likelihood = hessian(obj_func_likelihood)\\n    auto_hess_val = hess_obj_func_likelihood(theta)\\n\\n    # Store the results\\n    df.loc[t, 't'] = t\\n    df.loc[t, 'analytical_hess'] = auto_jacobian_val\\n    df.loc[t, 'auto_hess'] = auto_hess_val\\n    df.loc[t, 'equal'] = np.allclose(auto_jacobian_val, auto_hess_val)\\n\\n# Check the results\\nprint(sum(df['equal']) == N)\\ndf.tail()\\n\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Initialize the parameters\n",
    "theta = np.array([-0.2, 0.1])\n",
    "init_sigma2 = 0.5\n",
    "\n",
    "# Dataframe to store the results\n",
    "df = pd.DataFrame(columns=['t', 'analytical_hess', 'auto_hess', 'equal'])\n",
    "\n",
    "for t in range(N):\n",
    "    # Analytical gradient\n",
    "    obj_func_gradient = lambda param: analytical_grad_hess(param, init_sigma2)[0]['analytical_grad_at_i'][t]\n",
    "    jacobian_obj_func_gradient = jacobian(obj_func_gradient)\n",
    "    auto_jacobian_val = jacobian_obj_func_gradient(theta)\n",
    "\n",
    "    # Auto Hessian\n",
    "    obj_func_likelihood = lambda param: analytical_grad_hess(param, init_sigma2)[0]['log_likelihood_at_i'][t]\n",
    "    hess_obj_func_likelihood = hessian(obj_func_likelihood)\n",
    "    auto_hess_val = hess_obj_func_likelihood(theta)\n",
    "\n",
    "    # Store the results\n",
    "    df.loc[t, 't'] = t\n",
    "    df.loc[t, 'analytical_hess'] = auto_jacobian_val\n",
    "    df.loc[t, 'auto_hess'] = auto_hess_val\n",
    "    df.loc[t, 'equal'] = np.allclose(auto_jacobian_val, auto_hess_val)\n",
    "\n",
    "# Check the results\n",
    "print(sum(df['equal']) == N)\n",
    "df.tail()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b852827",
   "metadata": {},
   "source": [
    "# Test Hessian function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d939fa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using finite differences for Hessian computation (faster than jacobian)...\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZED HESSIAN COMPUTATION - FINITE DIFFERENCES\n",
    "print(\"Using finite differences for Hessian computation (faster than jacobian)...\")\n",
    "\n",
    "def compute_hessian_fast(thetaStar, init_sigma2):\n",
    "    \"\"\"Fast Hessian computation using finite differences\"\"\"\n",
    "    N = len(y)\n",
    "    p = len(thetaStar)\n",
    "    eps = 1e-5\n",
    "    \n",
    "    Hess_all = np.zeros((N, p, p))\n",
    "    \n",
    "    # Get base gradients\n",
    "    grad_result = analytical_grad_hess(thetaStar, init_sigma2)\n",
    "    grad_base = np.stack(grad_result[0]['analytical_grad_at_i'], axis=0)  # (N, p)\n",
    "    \n",
    "    for i in range(p):\n",
    "        theta_plus = thetaStar.copy()\n",
    "        theta_plus[i] += eps\n",
    "        \n",
    "        grad_plus_result = analytical_grad_hess(theta_plus, init_sigma2)\n",
    "        grad_plus = np.stack(grad_plus_result[0]['analytical_grad_at_i'], axis=0)\n",
    "        \n",
    "        # Finite difference approximation\n",
    "        Hess_all[:, i, :] = (grad_plus - grad_base) / eps\n",
    "        print(f\"Processed parameter {i+1}/{p}\")\n",
    "    \n",
    "    return Hess_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab26aa8",
   "metadata": {},
   "source": [
    "# Test Hessian function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76527415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport jax.numpy as jnp\\nimport jax\\nfrom jax import jit, vmap, grad, hessian\\nimport numpy as np\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import jit, vmap, grad, hessian\n",
    "import numpy as np\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "665f67d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# JAX version - nhanh hơn 10-50x\\ndef analytical_grad_hess_jax(theta, init_sigma2, y):\\n    JAX version of analytical_grad_hess - much faster\\n    N = len(y)\\n    \\n    # JAX-compatible Kalman filter implementation\\n    def kf_step(carry, t):\\n        x, V, dx, dV, sigma2_hat_sum, dsigma2_hat_sum = carry\\n        \\n        # Predict\\n        x_predict = F @ x\\n        V_predict = F @ V @ F.T + G @ G.T\\n        \\n        # Forecast error\\n        e_t = y[t] - (H @ x_predict)[0, 0]\\n        r_t = (H @ V_predict @ H.T)[0, 0]\\n        \\n        # Gradient calculations\\n        dx_predict = F @ dx + dF @ x\\n        dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + GdGT\\n        \\n        de_t = -H @ dx_predict\\n        dr_t = H @ dV_predict @ H.T\\n        \\n        # Update\\n        sigma2_hat_sum += e_t**2 / r_t\\n        sigma2_hat = sigma2_hat_sum / (t + 1)\\n        dsigma2_hat_sum += (2 * e_t * de_t) / r_t - (e_t**2 * dr_t) / (r_t**2)\\n        dsigma2_hat = dsigma2_hat_sum / (t + 1)\\n        \\n        # Kalman gain\\n        K = V_predict @ H.T / r_t\\n        x = x_predict + K * e_t\\n        V = (jnp.eye(k) - K @ H) @ V_predict\\n        \\n        dK = (dV_predict @ H.T / r_t) - (V_predict @ H.T / r_t**2) @ dr_t\\n        dx = dx_predict + K @ de_t + dK * e_t\\n        dV = dV_predict - dK @ H @ V_predict - K @ H @ dV_predict\\n        \\n        # Log-likelihood and gradient\\n        log_likelihood_at_i = -0.5 * (jnp.log(2 * jnp.pi) + jnp.log(sigma2_hat) + \\n                                     jnp.log(r_t) + e_t**2 / (r_t * sigma2_hat))\\n        \\n        analytical_grad_at_i = -0.5 * (dsigma2_hat / sigma2_hat + dr_t / r_t + \\n                                      (2 * e_t * sigma2_hat * r_t * de_t - \\n                                       e_t**2 * r_t * dsigma2_hat - \\n                                       e_t**2 * sigma2_hat * dr_t) / (sigma2_hat**2 * r_t**2))\\n        \\n        return (x, V, dx, dV, sigma2_hat_sum, dsigma2_hat_sum), (log_likelihood_at_i, analytical_grad_at_i.flatten())\\n    \\n    # Initialize\\n    F, G, H, dF, dG, V, dV, x, dx, k = initiate_KF_parameter_values(theta, init_sigma2)\\n    GdGT = jnp.array([G @ dG.T[0][i].reshape(1,2) for i in range(2)])\\n    \\n    # Run scan\\n    carry_init = (x, V, dx, dV, 0.0, jnp.zeros((2, 1, 1)))\\n    _, (log_likelihoods, gradients) = jax.lax.scan(kf_step, carry_init, jnp.arange(N))\\n    \\n    return {'log_likelihood_at_i': log_likelihoods, 'analytical_grad_at_i': gradients}\\n\\n# JIT compile for speed\\nanalytical_grad_hess_jax = jit(analytical_grad_hess_jax)\\n\\n# Vectorized Hessian calculation\\ndef compute_hessian_fast(thetaStar, init_sigma2, y):\\n    Fast Hessian calculation using JAX\\n    N = len(y)\\n    \\n    def grad_per_obs(theta):\\n        return analytical_grad_hess_jax(theta, init_sigma2, y)['analytical_grad_at_i']\\n    \\n    # Vectorized gradient function\\n    grads_all = vmap(grad_per_obs, in_axes=0)  \\n    \\n    # Compute Hessian for all observations at once\\n    Hess_all = jax.jacfwd(grads_all)(thetaStar)\\n    \\n    return Hess_all\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# JAX version - nhanh hơn 10-50x\n",
    "def analytical_grad_hess_jax(theta, init_sigma2, y):\n",
    "    \"\"\"\"\"\"JAX version of analytical_grad_hess - much faster\"\"\"\"\"\"\n",
    "    N = len(y)\n",
    "    \n",
    "    # JAX-compatible Kalman filter implementation\n",
    "    def kf_step(carry, t):\n",
    "        x, V, dx, dV, sigma2_hat_sum, dsigma2_hat_sum = carry\n",
    "        \n",
    "        # Predict\n",
    "        x_predict = F @ x\n",
    "        V_predict = F @ V @ F.T + G @ G.T\n",
    "        \n",
    "        # Forecast error\n",
    "        e_t = y[t] - (H @ x_predict)[0, 0]\n",
    "        r_t = (H @ V_predict @ H.T)[0, 0]\n",
    "        \n",
    "        # Gradient calculations\n",
    "        dx_predict = F @ dx + dF @ x\n",
    "        dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + GdGT\n",
    "        \n",
    "        de_t = -H @ dx_predict\n",
    "        dr_t = H @ dV_predict @ H.T\n",
    "        \n",
    "        # Update\n",
    "        sigma2_hat_sum += e_t**2 / r_t\n",
    "        sigma2_hat = sigma2_hat_sum / (t + 1)\n",
    "        dsigma2_hat_sum += (2 * e_t * de_t) / r_t - (e_t**2 * dr_t) / (r_t**2)\n",
    "        dsigma2_hat = dsigma2_hat_sum / (t + 1)\n",
    "        \n",
    "        # Kalman gain\n",
    "        K = V_predict @ H.T / r_t\n",
    "        x = x_predict + K * e_t\n",
    "        V = (jnp.eye(k) - K @ H) @ V_predict\n",
    "        \n",
    "        dK = (dV_predict @ H.T / r_t) - (V_predict @ H.T / r_t**2) @ dr_t\n",
    "        dx = dx_predict + K @ de_t + dK * e_t\n",
    "        dV = dV_predict - dK @ H @ V_predict - K @ H @ dV_predict\n",
    "        \n",
    "        # Log-likelihood and gradient\n",
    "        log_likelihood_at_i = -0.5 * (jnp.log(2 * jnp.pi) + jnp.log(sigma2_hat) + \n",
    "                                     jnp.log(r_t) + e_t**2 / (r_t * sigma2_hat))\n",
    "        \n",
    "        analytical_grad_at_i = -0.5 * (dsigma2_hat / sigma2_hat + dr_t / r_t + \n",
    "                                      (2 * e_t * sigma2_hat * r_t * de_t - \n",
    "                                       e_t**2 * r_t * dsigma2_hat - \n",
    "                                       e_t**2 * sigma2_hat * dr_t) / (sigma2_hat**2 * r_t**2))\n",
    "        \n",
    "        return (x, V, dx, dV, sigma2_hat_sum, dsigma2_hat_sum), (log_likelihood_at_i, analytical_grad_at_i.flatten())\n",
    "    \n",
    "    # Initialize\n",
    "    F, G, H, dF, dG, V, dV, x, dx, k = initiate_KF_parameter_values(theta, init_sigma2)\n",
    "    GdGT = jnp.array([G @ dG.T[0][i].reshape(1,2) for i in range(2)])\n",
    "    \n",
    "    # Run scan\n",
    "    carry_init = (x, V, dx, dV, 0.0, jnp.zeros((2, 1, 1)))\n",
    "    _, (log_likelihoods, gradients) = jax.lax.scan(kf_step, carry_init, jnp.arange(N))\n",
    "    \n",
    "    return {'log_likelihood_at_i': log_likelihoods, 'analytical_grad_at_i': gradients}\n",
    "\n",
    "# JIT compile for speed\n",
    "analytical_grad_hess_jax = jit(analytical_grad_hess_jax)\n",
    "\n",
    "# Vectorized Hessian calculation\n",
    "def compute_hessian_fast(thetaStar, init_sigma2, y):\n",
    "    \"\"\"\"\"\"Fast Hessian calculation using JAX\"\"\"\"\"\"\n",
    "    N = len(y)\n",
    "    \n",
    "    def grad_per_obs(theta):\n",
    "        return analytical_grad_hess_jax(theta, init_sigma2, y)['analytical_grad_at_i']\n",
    "    \n",
    "    # Vectorized gradient function\n",
    "    grads_all = vmap(grad_per_obs, in_axes=0)  \n",
    "    \n",
    "    # Compute Hessian for all observations at once\n",
    "    Hess_all = jax.jacfwd(grads_all)(thetaStar)\n",
    "    \n",
    "    return Hess_all\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32599a1",
   "metadata": {},
   "source": [
    "# Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d00ec78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# OPTIMIZED HESSIAN COMPUTATION - IMPROVED FINITE DIFFERENCES\\nprint(\"Using improved finite differences for Hessian computation...\")\\n\\ndef compute_hessian_fast(thetaStar, init_sigma2, method=\\'central\\', eps=1e-6):\\n    Improved Hessian computation using finite differences\\n    \\n    Parameters:\\n    - method: \\'forward\\', \\'central\\', or \\'adaptive\\'\\n    - eps: step size (will be optimized for adaptive method)\\n    \\n    N = len(y)\\n    p = len(thetaStar)\\n    \\n    Hess_all = np.zeros((N, p, p))\\n    \\n    # Get base gradients\\n    grad_result = analytical_grad_hess(thetaStar, init_sigma2)\\n    grad_base = np.stack(grad_result[0][\\'analytical_grad_at_i\\'], axis=0)  # (N, p)\\n    \\n    if method == \\'forward\\':\\n        # Forward difference (original method)\\n        for i in range(p):\\n            theta_plus = thetaStar.copy()\\n            theta_plus[i] += eps\\n            \\n            grad_plus_result = analytical_grad_hess(theta_plus, init_sigma2)\\n            grad_plus = np.stack(grad_plus_result[0][\\'analytical_grad_at_i\\'], axis=0)\\n            \\n            Hess_all[:, i, :] = (grad_plus - grad_base) / eps\\n            print(f\"Processed parameter {i+1}/{p} (forward difference)\")\\n    \\n    elif method == \\'central\\':\\n        # Central difference (more accurate)\\n        for i in range(p):\\n            theta_plus = thetaStar.copy()\\n            theta_minus = thetaStar.copy()\\n            theta_plus[i] += eps\\n            theta_minus[i] -= eps\\n            \\n            grad_plus_result = analytical_grad_hess(theta_plus, init_sigma2)\\n            grad_minus_result = analytical_grad_hess(theta_minus, init_sigma2)\\n            grad_plus = np.stack(grad_plus_result[0][\\'analytical_grad_at_i\\'], axis=0)\\n            grad_minus = np.stack(grad_minus_result[0][\\'analytical_grad_at_i\\'], axis=0)\\n            \\n            Hess_all[:, i, :] = (grad_plus - grad_minus) / (2 * eps)\\n            print(f\"Processed parameter {i+1}/{p} (central difference)\")\\n    \\n    elif method == \\'adaptive\\':\\n        # Adaptive step size for better accuracy\\n        for i in range(p):\\n            # Try different step sizes and choose the most stable one\\n            eps_candidates = [1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\\n            best_eps = eps_candidates[0]\\n            min_condition_number = float(\\'inf\\')\\n            \\n            for test_eps in eps_candidates:\\n                theta_plus = thetaStar.copy()\\n                theta_minus = thetaStar.copy()\\n                theta_plus[i] += test_eps\\n                theta_minus[i] -= test_eps\\n                \\n                grad_plus_result = analytical_grad_hess(theta_plus, init_sigma2)\\n                grad_minus_result = analytical_grad_hess(theta_minus, init_sigma2)\\n                grad_plus = np.stack(grad_plus_result[0][\\'analytical_grad_at_i\\'], axis=0)\\n                grad_minus = np.stack(grad_minus_result[0][\\'analytical_grad_at_i\\'], axis=0)\\n                \\n                hess_approx = (grad_plus - grad_minus) / (2 * test_eps)\\n                \\n                # Check condition number for stability\\n                try:\\n                    condition_num = np.linalg.cond(hess_approx)\\n                    if condition_num < min_condition_number:\\n                        min_condition_number = condition_num\\n                        best_eps = test_eps\\n                        Hess_all[:, i, :] = hess_approx\\n                except:\\n                    continue\\n            \\n            print(f\"Processed parameter {i+1}/{p} (adaptive, best eps={best_eps:.0e})\")\\n    \\n    return Hess_all\\n    '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# OPTIMIZED HESSIAN COMPUTATION - IMPROVED FINITE DIFFERENCES\n",
    "print(\"Using improved finite differences for Hessian computation...\")\n",
    "\n",
    "def compute_hessian_fast(thetaStar, init_sigma2, method='central', eps=1e-6):\n",
    "    Improved Hessian computation using finite differences\n",
    "    \n",
    "    Parameters:\n",
    "    - method: 'forward', 'central', or 'adaptive'\n",
    "    - eps: step size (will be optimized for adaptive method)\n",
    "    \n",
    "    N = len(y)\n",
    "    p = len(thetaStar)\n",
    "    \n",
    "    Hess_all = np.zeros((N, p, p))\n",
    "    \n",
    "    # Get base gradients\n",
    "    grad_result = analytical_grad_hess(thetaStar, init_sigma2)\n",
    "    grad_base = np.stack(grad_result[0]['analytical_grad_at_i'], axis=0)  # (N, p)\n",
    "    \n",
    "    if method == 'forward':\n",
    "        # Forward difference (original method)\n",
    "        for i in range(p):\n",
    "            theta_plus = thetaStar.copy()\n",
    "            theta_plus[i] += eps\n",
    "            \n",
    "            grad_plus_result = analytical_grad_hess(theta_plus, init_sigma2)\n",
    "            grad_plus = np.stack(grad_plus_result[0]['analytical_grad_at_i'], axis=0)\n",
    "            \n",
    "            Hess_all[:, i, :] = (grad_plus - grad_base) / eps\n",
    "            print(f\"Processed parameter {i+1}/{p} (forward difference)\")\n",
    "    \n",
    "    elif method == 'central':\n",
    "        # Central difference (more accurate)\n",
    "        for i in range(p):\n",
    "            theta_plus = thetaStar.copy()\n",
    "            theta_minus = thetaStar.copy()\n",
    "            theta_plus[i] += eps\n",
    "            theta_minus[i] -= eps\n",
    "            \n",
    "            grad_plus_result = analytical_grad_hess(theta_plus, init_sigma2)\n",
    "            grad_minus_result = analytical_grad_hess(theta_minus, init_sigma2)\n",
    "            grad_plus = np.stack(grad_plus_result[0]['analytical_grad_at_i'], axis=0)\n",
    "            grad_minus = np.stack(grad_minus_result[0]['analytical_grad_at_i'], axis=0)\n",
    "            \n",
    "            Hess_all[:, i, :] = (grad_plus - grad_minus) / (2 * eps)\n",
    "            print(f\"Processed parameter {i+1}/{p} (central difference)\")\n",
    "    \n",
    "    elif method == 'adaptive':\n",
    "        # Adaptive step size for better accuracy\n",
    "        for i in range(p):\n",
    "            # Try different step sizes and choose the most stable one\n",
    "            eps_candidates = [1e-4, 1e-5, 1e-6, 1e-7, 1e-8]\n",
    "            best_eps = eps_candidates[0]\n",
    "            min_condition_number = float('inf')\n",
    "            \n",
    "            for test_eps in eps_candidates:\n",
    "                theta_plus = thetaStar.copy()\n",
    "                theta_minus = thetaStar.copy()\n",
    "                theta_plus[i] += test_eps\n",
    "                theta_minus[i] -= test_eps\n",
    "                \n",
    "                grad_plus_result = analytical_grad_hess(theta_plus, init_sigma2)\n",
    "                grad_minus_result = analytical_grad_hess(theta_minus, init_sigma2)\n",
    "                grad_plus = np.stack(grad_plus_result[0]['analytical_grad_at_i'], axis=0)\n",
    "                grad_minus = np.stack(grad_minus_result[0]['analytical_grad_at_i'], axis=0)\n",
    "                \n",
    "                hess_approx = (grad_plus - grad_minus) / (2 * test_eps)\n",
    "                \n",
    "                # Check condition number for stability\n",
    "                try:\n",
    "                    condition_num = np.linalg.cond(hess_approx)\n",
    "                    if condition_num < min_condition_number:\n",
    "                        min_condition_number = condition_num\n",
    "                        best_eps = test_eps\n",
    "                        Hess_all[:, i, :] = hess_approx\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"Processed parameter {i+1}/{p} (adaptive, best eps={best_eps:.0e})\")\n",
    "    \n",
    "    return Hess_all\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4182c36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using improved finite differences for Hessian computation...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# OPTIMIZED HESSIAN COMPUTATION - IMPROVED FINITE DIFFERENCES\n",
    "print(\"Using improved finite differences for Hessian computation...\")\n",
    "\n",
    "def compute_hessian_fast(thetaStar, init_sigma2, eps=1e-6):\n",
    "    \"\"\"Improved Hessian computation using finite differences\n",
    "    \n",
    "    Parameters:\n",
    "    - method: 'forward', 'central', or 'adaptive'\n",
    "    - eps: step size (will be optimized for adaptive method)\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    p = len(thetaStar)\n",
    "    \n",
    "    Hess_all = np.zeros((N, p, p))\n",
    "    \n",
    "    # Get base gradients\n",
    "    grad_result = analytical_grad_hess(thetaStar, init_sigma2)\n",
    "    grad_base = np.stack(grad_result[0]['analytical_grad_at_i'], axis=0)  # (N, p)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Central difference (more accurate)\n",
    "    for i in range(p):\n",
    "        theta_plus = thetaStar.copy()\n",
    "        theta_minus = thetaStar.copy()\n",
    "        theta_plus[i] += eps\n",
    "        theta_minus[i] -= eps\n",
    "        \n",
    "        grad_plus_result = analytical_grad_hess(theta_plus, init_sigma2)\n",
    "        grad_minus_result = analytical_grad_hess(theta_minus, init_sigma2)\n",
    "        grad_plus = np.stack(grad_plus_result[0]['analytical_grad_at_i'], axis=0)\n",
    "        grad_minus = np.stack(grad_minus_result[0]['analytical_grad_at_i'], axis=0)\n",
    "        \n",
    "        Hess_all[:, i, :] = (grad_plus - grad_minus) / (2 * eps)\n",
    "        print(f\"Processed parameter {i+1}/{p} (central difference)\")\n",
    "\n",
    "    \n",
    "    return Hess_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07cdcb",
   "metadata": {},
   "source": [
    "# Compare analytical and auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9389681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed parameter 1/2 (central difference)\n",
      "Processed parameter 2/2 (central difference)\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>analytical_hess</th>\n",
       "      <th>auto_hess</th>\n",
       "      <th>equal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>395</td>\n",
       "      <td>[[-1.840652329720971, 1.5684698907447725], [1....</td>\n",
       "      <td>[[-1.8406523306568052, 1.5684698915351203], [1...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>396</td>\n",
       "      <td>[[2.3585653845803467, -4.5456003926469535], [-...</td>\n",
       "      <td>[[2.3585653808772804, -4.545600391200961], [-4...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>397</td>\n",
       "      <td>[[-3.879118639416035, 4.937297475360314], [4.9...</td>\n",
       "      <td>[[-3.879118638464531, 4.937297475045555], [4.9...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>398</td>\n",
       "      <td>[[-0.06358695983643692, -1.2168260100087203], ...</td>\n",
       "      <td>[[-0.06358695970666012, -1.2168260095861094], ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>399</td>\n",
       "      <td>[[-0.017521384898522285, 0.21225551005699117],...</td>\n",
       "      <td>[[-0.017521384025945472, 0.2122555096551761], ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       t                                    analytical_hess  \\\n",
       "395  395  [[-1.840652329720971, 1.5684698907447725], [1....   \n",
       "396  396  [[2.3585653845803467, -4.5456003926469535], [-...   \n",
       "397  397  [[-3.879118639416035, 4.937297475360314], [4.9...   \n",
       "398  398  [[-0.06358695983643692, -1.2168260100087203], ...   \n",
       "399  399  [[-0.017521384898522285, 0.21225551005699117],...   \n",
       "\n",
       "                                             auto_hess equal  \n",
       "395  [[-1.8406523306568052, 1.5684698915351203], [1...  True  \n",
       "396  [[2.3585653808772804, -4.545600391200961], [-4...  True  \n",
       "397  [[-3.879118638464531, 4.937297475045555], [4.9...  True  \n",
       "398  [[-0.06358695970666012, -1.2168260095861094], ...  True  \n",
       "399  [[-0.017521384025945472, 0.2122555096551761], ...  True  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the parameters\n",
    "theta = np.array([-0.2, 0.1])\n",
    "init_sigma2 = 0.5\n",
    "\n",
    "# Dataframe to store the results\n",
    "df = pd.DataFrame(columns=['t', 'analytical_hess', 'auto_hess', 'equal'])\n",
    "\n",
    "# Hàm trả về ma trận [N, p] các gradient theo từng t\n",
    "# grads_all = lambda param: np.stack(\n",
    "#     analytical_grad_hess(param, init_sigma2=0.5)[0]['analytical_grad_at_i'], axis=0\n",
    "# ) \n",
    "\n",
    "# Hess_all_at_thetaStar = jacobian(grads_all)(theta)\n",
    "Hess_all_at_thetaStar = compute_hessian_fast(theta, init_sigma2)\n",
    "\n",
    "\n",
    "for t in range(N):\n",
    "    # Analytical gradient\n",
    "    # obj_func_gradient = lambda param: analytical_grad_hess(param, init_sigma2)[0]['analytical_grad_at_i'][t]\n",
    "    # jacobian_obj_func_gradient = jacobian(obj_func_gradient)\n",
    "    auto_jacobian_val = Hess_all_at_thetaStar[t]\n",
    "\n",
    "    # Auto Hessian\n",
    "    obj_func_likelihood = lambda param: analytical_grad_hess(param, init_sigma2)[0]['log_likelihood_at_i'][t]\n",
    "    hess_obj_func_likelihood = hessian(obj_func_likelihood)\n",
    "    auto_hess_val = hess_obj_func_likelihood(theta)\n",
    "\n",
    "    # Store the results\n",
    "    df.loc[t, 't'] = t\n",
    "    df.loc[t, 'analytical_hess'] = auto_jacobian_val\n",
    "    df.loc[t, 'auto_hess'] = auto_hess_val\n",
    "    df.loc[t, 'equal'] = np.allclose(auto_jacobian_val, auto_hess_val)\n",
    "\n",
    "# Check the results\n",
    "print(sum(df['equal']) == N)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b16cb1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['equal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b26f43",
   "metadata": {},
   "source": [
    "# Control variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_control_variate_quantities(thetaStar, init_sigma2=0.5):\n",
    "    \"\"\"\n",
    "    Creates the quantities needed to construct the second order parameter expanded Taylor control variates for the log_density.\n",
    "    Output from this function will go into the function eval_q_k\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    p = len(thetaStar)\n",
    "\n",
    "    # Run the Kalman filter for gradient\n",
    "    analytical_grad_hess_result = analytical_grad_hess(thetaStar, init_sigma2)\n",
    "    # dens_at_thetaStar = analytical_grad_hess_result[2]\n",
    "    dens_at_thetaStar = analytical_grad_hess_result[0]['log_likelihood_at_i']\n",
    "    grad_dens = analytical_grad_hess_result[0]['analytical_grad_at_i']\n",
    "    grad_at_thetaStar = np.zeros([n, p])\n",
    "    Hess_at_thetaStar = np.zeros([n, p, p])\n",
    "\n",
    "    Hess_dens = compute_hessian_fast(thetaStar, init_sigma2)\n",
    "    \n",
    "    for i in range(n):    \n",
    "        if i % 100 == 0:\n",
    "            print(\"Processed %s observations (out of % s)\" % (i, n))\n",
    "\n",
    "\n",
    "        # Run the analytical gradient and hessian\n",
    "        # obj_grad_dens = lambda param: analytical_grad_hess(param, init_sigma2)[0]['analytical_grad_at_i'][i]\n",
    "        # Hess_dens = jacobian(obj_grad_dens)\n",
    "\n",
    "        grad_at_thetaStar[i, :], Hess_at_thetaStar[i, :] = grad_dens[i], Hess_dens[i]\n",
    "                    \n",
    "    return dens_at_thetaStar, grad_at_thetaStar, Hess_at_thetaStar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aaefabb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntheta_optim_start = np.array([a, b])\\nobj_func_posterior = lambda param: analytical_grad_hess(param, init_sigma2=0.5)[2]\\ngrad_obj_func_posterior = lambda param: analytical_grad_hess(param, init_sigma2=0.5)[0]['analytical_grad_at_i'][N-1]\\n\\nres_optim_posterior = minimize(obj_func_posterior, \\n                                theta_optim_start, \\n                                method='BFGS', \\n                                # jac=grad_obj_func_posterior, \\n                                options={'gtol': 1e-04, 'maxiter': 1000, 'disp': True})\\nres_optim_posterior.x\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "theta_optim_start = np.array([a, b])\n",
    "obj_func_posterior = lambda param: analytical_grad_hess(param, init_sigma2=0.5)[2]\n",
    "grad_obj_func_posterior = lambda param: analytical_grad_hess(param, init_sigma2=0.5)[0]['analytical_grad_at_i'][N-1]\n",
    "\n",
    "res_optim_posterior = minimize(obj_func_posterior, \n",
    "                                theta_optim_start, \n",
    "                                method='BFGS', \n",
    "                                # jac=grad_obj_func_posterior, \n",
    "                                options={'gtol': 1e-04, 'maxiter': 1000, 'disp': True})\n",
    "res_optim_posterior.x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694661e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 observations (out of 400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x128e79cd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/thangtm589/Desktop/study/UTS/35112 Mathematical Research/winston-uts-research/venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "thetaStar = np.array([-a-0.001, -b+0.001])\n",
    "dens_at_thetaStar_Poisson, grad_at_thetaStar_Poisson, Hess_at_thetaStar_Poisson = initiate_control_variate_quantities(thetaStar, init_sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e754c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_q_k(theta, dens_at_thetaStar, grad_at_thetaStar, Hess_at_thetaStar, order = 2):\n",
    "    \"\"\"\n",
    "    Evaluates the order order parameter expanded Taylor control variates at the point theta for all observations in \n",
    "    dens_at_thetaStar, grad_at_thetaStar, Hess_at_thetaStar. Default order is 2.\n",
    "    \"\"\"\n",
    "    const_term = dens_at_thetaStar\n",
    "    if order == 0:\n",
    "        q_k = const_term\n",
    "    elif order == 1:\n",
    "        first_term = np.sum(grad_at_thetaStar*(theta - thetaStar), axis = 1)\n",
    "        q_k = const_term + first_term\n",
    "    elif order == 2:\n",
    "        first_term = np.sum(grad_at_thetaStar*(theta - thetaStar), axis = 1)\n",
    "        second_term = 0.5*np.sum(np.sum(Hess_at_thetaStar*np.outer(theta - thetaStar, theta - thetaStar), axis = 1), axis = 1)    \n",
    "        q_k = const_term + first_term + second_term\n",
    "    else:\n",
    "        raise ValueError(\"Order must be 0 <= order <= 2\")\n",
    "    return q_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a3d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sigma = np.array([[0.1, 0.05], [0.05, 0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ca39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dens_at_thetaStar, grad_at_thetaStar, Hess_at_thetaStar = dens_at_thetaStar_Poisson, grad_at_thetaStar_Poisson, Hess_at_thetaStar_Poisson\n",
    "theta = np.random.multivariate_normal(thetaStar, test_sigma, size = 1).flatten()\n",
    "q_k_order0 = eval_q_k(theta, dens_at_thetaStar, grad_at_thetaStar, Hess_at_thetaStar, order = 0) \n",
    "q_k_order1 = eval_q_k(theta, dens_at_thetaStar, grad_at_thetaStar, Hess_at_thetaStar, order = 1) \n",
    "q_k_order2 = eval_q_k(theta, dens_at_thetaStar, grad_at_thetaStar, Hess_at_thetaStar, order = 2) \n",
    "l_k = analytical_grad_hess(theta, init_sigma2=0.5)[0]['log_likelihood_at_i']\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.suptitle(\n",
    "    r'Control variate at $\\theta$ such that $||\\theta - \\theta^\\star|| = %3.4f$' \n",
    "    % np.linalg.norm(theta - thetaStar)\n",
    "    + \"\\nSample size = %s\" % N,\n",
    "    size=14,\n",
    "    y = 1.02\n",
    ")\n",
    "# fig.subplots_adjust(top=0.2)\n",
    "\n",
    "for k in range(3):\n",
    "        axs[k].plot(l_k, l_k, color = 'red')\n",
    "        if k == 0:\n",
    "            axs[k].plot(l_k, q_k_order0, '.', color = 'blue')\n",
    "            axs[k].set_title('Order 0', size = 12)\n",
    "        elif k == 1:\n",
    "            axs[k].plot(l_k, q_k_order1, '.', color = 'blue')\n",
    "            axs[k].set_title('Order 1', size = 12)\n",
    "        elif k == 2:\n",
    "            axs[k].plot(l_k, q_k_order2, '.', color = 'blue')\n",
    "            axs[k].set_title('Order 2', size = 12)\n",
    "            \n",
    "        axs[k].axis('scaled')\n",
    "\n",
    "fig.subplots_adjust(top=1.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
