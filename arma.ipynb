{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib, time, copy\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import autograd.scipy.stats as sps_autograd\n",
    "from autograd import grad, hessian\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import toeplitz\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate ARMA data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simulate ARMA(1, 1) model\n",
    "\"\"\"\n",
    "\n",
    "# Define AR and MA coefficients\n",
    "ar = np.array([1, -0.5])  \n",
    "ma = np.array([1, 0.4])        \n",
    "\n",
    "# Create ARMA process object\n",
    "arma_process = ArmaProcess(ar, ma)\n",
    "\n",
    "# Simulate 10000 samples\n",
    "N = 1000\n",
    "y = arma_process.generate_sample(nsample=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karman Filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_FGHQ(a, b):\n",
    "    \"\"\"\n",
    "    Construct the state-space matrices F, G, H for an ARMA(p, q) model.\n",
    "\n",
    "    Parameters:\n",
    "    - a: list or np.array of AR coefficients [a1, a2, ..., ap]\n",
    "    - b: list or np.array of MA coefficients [b1, b2, ..., bq]\n",
    "\n",
    "    Returns:\n",
    "    - F: state transition matrix of shape (k, k)\n",
    "    - G: noise coefficient matrix of shape (k, 1)\n",
    "    - H: observation matrix of shape (1, k)\n",
    "    - Q: covariance identity matrix of shape (k, k)\n",
    "    - dF: derivative of F with respect to theta of shape (l, k, k)\n",
    "    - dG: derivative of G with respect to theta of shape (l, k, 1)\n",
    "    \"\"\"\n",
    "    p = len(a)\n",
    "    q = len(b)\n",
    "    k = max(p, q + 1)  # dimension of the state vector\n",
    "    F = np.zeros((k, k))\n",
    "    G = np.zeros((k, 1))\n",
    "    H = np.zeros((1, k))\n",
    "\n",
    "    # Fill the first column of F with AR coefficients\n",
    "    for i in range(p):\n",
    "        F[i, 0] = a[i]\n",
    "\n",
    "    # Fill the lower subdiagonal of F with 1s (shifting the state)\n",
    "    for i in range(k - 1):\n",
    "        F[i, i + 1] = 1\n",
    "\n",
    "    # Fill G with negative MA coefficients, first element = 1\n",
    "    for i in range(q):\n",
    "        G[i+1, 0] = -b[i]\n",
    "    G[0, 0] = 1  # first element always set to 1\n",
    "\n",
    "    # Matrix H: only first element is 1\n",
    "    H[0, 0] = 1\n",
    "\n",
    "    # Initialize covariance matrix Q as identity matrix\n",
    "    Q = np.eye(k)\n",
    "\n",
    "    # Compute derivatives of F and G with respect to theta\n",
    "    dF = np.zeros((k, k, k))\n",
    "    dF[0, 0, 0] = 1 # ARMA(1,1)\n",
    "\n",
    "    dG = np.zeros((k, k, 1))\n",
    "    dG[1, 1, 0] = -1 # ARMA(1,1)\n",
    "\n",
    "    return F, G, H, Q, dF, dG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(sigma2_hat, r):\n",
    "    N = len(r)\n",
    "    return -0.5 * (N * np.log(2 * np.pi) \n",
    "                   + N * np.log(sigma2_hat) \n",
    "                   + np.sum(np.log(r)) + N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def karman_filter_arma(theta):\n",
    "    p = 1   # AR order\n",
    "    q = 1   # MA order\n",
    "    a = theta[:p]\n",
    "    b = theta[p:p+q]\n",
    "    k = max(p, q + 1)\n",
    "    \n",
    "    F, G, H, Q, dF, dG = initialize_FGHQ(a, b)\n",
    "\n",
    "    # Initialize values\n",
    "    x = np.zeros((k, 1))\n",
    "    V = np.eye(k) * 100\n",
    "    e = np.zeros((N, 1))\n",
    "    r = np.zeros((N, 1))\n",
    "\n",
    "    # Implement Kalman filter\n",
    "    for t in range(N):\n",
    "        # Predict one-step-ahead state predictive density of x_{t}\n",
    "        x_predict = F @ x\n",
    "        V_predict = F @ V @ F.T + G @ G.T\n",
    "\n",
    "        # Compute forecast error and one-step-ahead predictive variance\n",
    "        e[t] = y[t] - (H @ x_predict).item()\n",
    "        r[t] = (H @ V_predict @ H.T).item()\n",
    "\n",
    "        # Kalman gain\n",
    "        K = V_predict @ H.T / r[t]\n",
    "\n",
    "        # Update current state and covariance\n",
    "        x = x_predict + K * e[t]\n",
    "        V = (np.eye(k) - K @ H) @ V_predict\n",
    "\n",
    "    sigma2_hat = np.sum(e**2 / r) / N\n",
    "\n",
    "    return sigma2_hat, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_func_likelihood(theta):\n",
    "    sigma2_hat, r = karman_filter_arma(theta)\n",
    "    log_lik = log_likelihood(sigma2_hat, r)\n",
    "\n",
    "    return -log_lik  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated parameters: [ 0.49753443 -0.40805501]\n",
      "Negative log-likelihood: 1383.1985150906685\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test the log-likelihood function\n",
    "\"\"\"\n",
    "\n",
    "theta_start = [0.1, 0.1]\n",
    "\n",
    "# Minimize negative log-likelihood\n",
    "result = minimize(obj_func_likelihood, theta_start, method='BFGS')\n",
    "\n",
    "# Print results\n",
    "print(\"Estimated parameters:\", result.x)\n",
    "print(\"Negative log-likelihood:\", result.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm for the gradient of the model ARMA(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_initial_V_and_dV(a, b, sigma2):\n",
    "    \"\"\"\n",
    "    Compute the initial V and the initial dV/d(theta).\n",
    "    Implement the equations from Section 3.3.2 of the document.\n",
    "    Parameters:\n",
    "        a: float\n",
    "        b: float\n",
    "        sigma2: float (randomly chosen)\n",
    "    Returns:\n",
    "        V: numpy array of shape (k, k)\n",
    "        dV: numpy array of shape (k, k, k)\n",
    "    \"\"\"\n",
    "    p, q = 1, 1\n",
    "    k = max(p, q + 1)  \n",
    "\n",
    "    # 1. Compute initial V \n",
    "    g = np.zeros(k + 1)\n",
    "    g[0] = 1.0\n",
    "    g[1] = a - b\n",
    "    g[2] = a * g[1]\n",
    "\n",
    "    C = np.zeros(k + 1)\n",
    "    C[0] = sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)\n",
    "    C[1] = a * C[0] - sigma2 * b\n",
    "    C[2] = a * C[1]\n",
    "\n",
    "    V = np.zeros((k, k))\n",
    "    V[0, 0] = C[0]\n",
    "    V[0, 1] = V[1, 0] = - b * g[0]\n",
    "    V[1, 1] = b**2 * sigma2\n",
    "\n",
    "    dV = np.zeros((k, k, k))\n",
    "    dV[0, 0, 0] = (2 * sigma2 * (a-b) * (1 - a*b)) / (1 - a**2)**2\n",
    "    dV[0, 0, 1] = 0\n",
    "    dV[0, 1, 0] = 0\n",
    "    dV[0, 1, 1] = 0\n",
    "    dV[1, 0, 0] = 2 * sigma2 * (b-a) / (1 - a**2)\n",
    "    dV[1, 0, 1] = -1\n",
    "    dV[1, 1, 0] = -1\n",
    "    dV[1, 1, 1] = 2 * b * sigma2\n",
    "\n",
    "    return V, dV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_obj_func_likelihood(theta):\n",
    "    \"\"\"\n",
    "    Implement the gradient of the log-likelihood function for ARMA(1,1) model.\n",
    "    \"\"\"\n",
    "    \n",
    "    a, b = theta\n",
    "    k = 2\n",
    "\n",
    "    \"\"\" \n",
    "    # Initialize by function\n",
    "    # Initialize the state-space matrices\n",
    "    F, G, H, Q, dF, dG = initialize_FGHQ(np.array([a]), np.array([b]))\n",
    "    \n",
    "    # Initialize the x and V matrices\n",
    "    V, dV = compute_initial_V_and_dV(a, b, 100)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize by hand\n",
    "    sigma2 = 100\n",
    "    # Simple operations that autograd can handle\n",
    "    F = np.array([[a, 1.0], [0.0, 0.0]])\n",
    "    G = np.array([[1.0], [-b]])\n",
    "    H = np.array([[1.0, 0.0]])\n",
    "    dF = np.array([\n",
    "        [[1, 0], [0, 0]],  # First parameter (AR coefficient)\n",
    "        [[0, 0], [0, 0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dF.reshape(2, 2, 2)\n",
    "    dG = np.array([\n",
    "        [[0], [0]],  # First parameter (AR coefficient)\n",
    "        [[0], [-1]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dG.reshape(2, 2, 1)\n",
    "\n",
    "    g = np.array([1.0, a - b, a * (a - b)])\n",
    "\n",
    "    C = np.array([\n",
    "        sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2),\n",
    "        a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b,\n",
    "        a * (a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b)\n",
    "    ])\n",
    "\n",
    "    V = np.array([\n",
    "        [C[0], -b * g[0]],\n",
    "        [-b * g[0], b**2 * sigma2]\n",
    "    ])\n",
    "\n",
    "    dV = np.array([\n",
    "        [[(2 * sigma2 * (a-b) * (1 - a*b)) / (1 - a**2)**2, 0],\n",
    "        [0, 0]],\n",
    "        [[2 * sigma2 * (b-a) / (1 - a**2), -1],\n",
    "        [-1, 2 * b * sigma2]]\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    x = np.zeros((k, 1))\n",
    "    dx = np.zeros((2, 2, 1))\n",
    "    e = np.zeros((N, 1))\n",
    "    r = np.zeros((N, 1))\n",
    "    de = np.zeros((N, 2))\n",
    "    dr = np.zeros((N, 2))\n",
    "\n",
    "    # \n",
    "    for t in range(N):\n",
    "        # 1. Predict\n",
    "        # Predict one-step-ahead state predictive density of x_{t}\n",
    "        x_predict = F @ x\n",
    "        V_predict = F @ V @ F.T + G @ G.T\n",
    "\n",
    "        # Compute forecast error and one-step-ahead predictive variance\n",
    "        e[t] = y[t] - (H @ x_predict)[0, 0]\n",
    "        r[t] = (H @ V_predict @ H.T)[0, 0]\n",
    "\n",
    "        GdGT = np.array([G @ dG.T[0][i].reshape(1,2) for i in range(2)])\n",
    "\n",
    "        # Kalman filter for gradient\n",
    "        dx_predict = F @ dx + dF @ x\n",
    "        dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + GdGT\n",
    "\n",
    "        # Calculate de_t and dr_t as tensor(2,1,1)\n",
    "        de_t = -H @ dx_predict\n",
    "        dr_t = H @ dV_predict @ H.T\n",
    "\n",
    "\n",
    "        # 2. Update\n",
    "        # Kalman gain\n",
    "        K = V_predict @ H.T / r[t]\n",
    "\n",
    "        # Update current state and covariance\n",
    "        x = x_predict + K * e[t]\n",
    "        V = (np.eye(k) - K @ H) @ V_predict\n",
    "\n",
    "        # dK = (dV_predict @ H.T / r[t]) - np.einsum('ki,p->pki', (V_predict @ H.T / r[t]**2), dr_t)\n",
    "        dK = (dV_predict @ H.T / r[t]) - (V_predict @ H.T / r[t]**2) @ dr_t\n",
    "        dx = dx_predict + K @ de_t + dK * e[t]\n",
    "        dV = dV_predict - dK @ H @ V_predict - K @ H @ dV_predict\n",
    "\n",
    "        # Store value de and dr into vectors\n",
    "        de[t] = de_t.flatten()\n",
    "        dr[t] = dr_t.flatten()\n",
    "\n",
    "    # Compute sigma2_hat and gradient of the log-likelihood\n",
    "    sigma2_hat = np.sum(e**2 / r) / N\n",
    "    \n",
    "    grad = -0.5 * sum(dr / r) - (1/sigma2_hat) * sum(de * e / r) + (1/(2*sigma2_hat)) * sum(dr * e**2 / r**2)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 1748.331548\n",
      "         Iterations: 0\n",
      "         Function evaluations: 56\n",
      "         Gradient evaluations: 44\n",
      "Estimated parameters: [0.1 0.1]\n",
      "Negative log-likelihood: 1748.3315475368995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thangtm589/Desktop/study/UTS/35112 Mathematical Research/winston-uts-research/venv/lib/python3.11/site-packages/scipy/optimize/_minimize.py:733: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test the log-likelihood function\n",
    "\"\"\"\n",
    "\n",
    "theta_start = [0.1, 0.1]\n",
    "\n",
    "# Minimize negative log-likelihood\n",
    "result = minimize(obj_func_likelihood, theta_start, method='BFGS', jac=grad_obj_func_likelihood,\n",
    "                  options={'gtol': 1e-04, 'maxiter': 1000, 'disp': True})\n",
    "\n",
    "# Print results\n",
    "print(\"Estimated parameters:\", result.x)\n",
    "print(\"Negative log-likelihood:\", result.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/_wg4l14d02v5d6jkm_cpp_6w0000gn/T/ipykernel_891/2869779034.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  sigma2_hat = np.sum(e**2 / r) / N\n",
      "/var/folders/8v/_wg4l14d02v5d6jkm_cpp_6w0000gn/T/ipykernel_891/2869779034.py:61: RuntimeWarning: invalid value encountered in divide\n",
      "  grad = -0.5 * sum(dr / r) - (1/sigma2_hat) * sum(de * e / r) + (1/(2*sigma2_hat)) * sum(dr * e**2 / r**2)\n"
     ]
    }
   ],
   "source": [
    "a = 0.3\n",
    "b = 0.5\n",
    "k = 2\n",
    "\n",
    "# Initialize the state-space matrices\n",
    "F, G, H, Q, dF, dG = initialize_FGHQ(np.array([a]), np.array([b]))\n",
    "\n",
    "# Initialize the x and V matrices\n",
    "V, dV = compute_initial_V_and_dV(a, b, 100)\n",
    "x = np.zeros((k, 1))\n",
    "dx = np.zeros((2, 2, 1))\n",
    "e = np.zeros((N, 1))\n",
    "r = np.zeros((N, 1))\n",
    "de = np.zeros((N, 2))\n",
    "dr = np.zeros((N, 2))\n",
    "\n",
    "\n",
    "# \n",
    "t = 0\n",
    "    \n",
    "# 1. Predict\n",
    "# Predict one-step-ahead state predictive density of x_{t}\n",
    "x_predict = F @ x\n",
    "V_predict = F @ V @ F.T + G @ G.T\n",
    "\n",
    "# Compute forecast error and one-step-ahead predictive variance\n",
    "e[t] = y[t] - (H @ x_predict).item()\n",
    "r[t] = (H @ V_predict @ H.T).item()\n",
    "\n",
    "GdGT = np.array([G @ dG.T[0][i].reshape(1,2) for i in range(2)])\n",
    "\n",
    "# Kalman filter for gradient\n",
    "dx_predict = F @ dx + dF @ x\n",
    "dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + GdGT\n",
    "\n",
    "# Calculate de_t and dr_t as tensor(2,1,1)\n",
    "de_t = -H @ dx_predict\n",
    "dr_t = H @ dV_predict @ H.T\n",
    "\n",
    "\n",
    "# 2. Update\n",
    "# Kalman gain\n",
    "K = V_predict @ H.T / r[t]\n",
    "\n",
    "# Update current state and covariance\n",
    "x = x_predict + K * e[t]\n",
    "V = (np.eye(k) - K @ H) @ V_predict\n",
    "\n",
    "# dK = (dV_predict @ H.T / r[t]) - np.einsum('ki,p->pki', (V_predict @ H.T / r[t]**2), dr_t)\n",
    "dK = (dV_predict @ H.T / r[t]) - (V_predict @ H.T / r[t]**2) @ dr_t\n",
    "dx = dx_predict + K @ de_t + dK * e[t]\n",
    "dV = dV_predict - dK @ H @ V_predict - K @ H @ dV_predict\n",
    "\n",
    "# Store value de and dr \n",
    "de[t] = de_t.flatten()\n",
    "dr[t] = dr_t.flatten()\n",
    "\n",
    "# Compute the gradient of the log-likelihood function\n",
    "sigma2_hat = np.sum(e**2 / r) / N\n",
    "\n",
    "grad = -0.5 * sum(dr / r) - (1/sigma2_hat) * sum(de * e / r) + (1/(2*sigma2_hat)) * sum(dr * e**2 / r**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02352118561015082"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dK[0, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.22044605e-16],\n",
       "        [2.35211856e-02]],\n",
       "\n",
       "       [[4.44089210e-16],\n",
       "        [1.34630170e-02]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3, 1. ],\n",
       "       [0. , 0. ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ],\n",
       "       [-0.5]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.       ],\n",
       "       [-0.0142468]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd gradient: 0.023521185610150815\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "\n",
    "def test(a, b, sigma2):\n",
    "    \"\"\"Version using array indexing instead of .at\"\"\"\n",
    "    b = 0.5\n",
    "\n",
    "    # Create arrays using autograd-compatible operations\n",
    "    g = np.array([1.0, a - b, a * (a - b)])\n",
    "    \n",
    "    sigma2 = 100\n",
    "    C = np.array([\n",
    "        sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2),\n",
    "        a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b,\n",
    "        a * (a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b)\n",
    "    ])\n",
    "\n",
    "    V = np.array([\n",
    "        [C[0], -b * g[0]],\n",
    "        [-b * g[0], b**2 * sigma2]\n",
    "    ])\n",
    "    \n",
    "    # Simple operations that autograd can handle\n",
    "    F = np.array([[a, 1.0], [0.0, 0.0]])\n",
    "    G = np.array([[1.0], [-b]])\n",
    "    H = np.array([[1.0, 0.0]])\n",
    "    \n",
    "    x = np.zeros((2, 1))\n",
    "    \n",
    "    # One step of Kalman filter\n",
    "    x_predict = F @ x\n",
    "    V_predict = F @ V @ F.T + G @ G.T\n",
    "    \n",
    "    # Use autograd-compatible operations\n",
    "    e_t = y[0] - (H @ x_predict)[0, 0]\n",
    "    r_t = (H @ V_predict @ H.T)[0, 0]\n",
    "    \n",
    "    K = V_predict @ H.T / r_t\n",
    "    \n",
    "    # Return a scalar value\n",
    "    return K[1, 0]\n",
    "\n",
    "# Test\n",
    "from autograd import grad\n",
    "test_grad = lambda a: test(a, b, sigma2=100)\n",
    "grad_func = grad(test_grad)\n",
    "result = grad_func(0.3)\n",
    "print(\"Autograd gradient:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to compare derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autograd gradient t=0: 0.06070110904023112\n",
      "Analytical gradient t=0: 0.06070110904023111\n",
      "--------------------------------\n",
      "Autograd gradient t=1: 0.03453919699603452\n",
      "Analytical gradient t=1: 0.03453919699603451\n",
      "--------------------------------\n",
      "Autograd gradient t=2: 0.07177758330309453\n",
      "Analytical gradient t=2: 0.07177758330309453\n",
      "--------------------------------\n",
      "Autograd gradient t=3: -0.08938646998851754\n",
      "Analytical gradient t=3: -0.08938646998851753\n",
      "--------------------------------\n",
      "Autograd gradient t=4: -0.16890745483048453\n",
      "Analytical gradient t=4: -0.1689074548304845\n",
      "--------------------------------\n",
      "Autograd gradient t=5: -0.11145823716143788\n",
      "Analytical gradient t=5: -0.11145823716143786\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implement the gradient for AR.\n",
    "\"\"\"\n",
    "def a_grad(a, b):\n",
    "    k = 2\n",
    "    sigma2 = 100\n",
    "\n",
    "    # Simple operations that autograd can handle\n",
    "    F = np.array([[a, 1.0], [0.0, 0.0]])\n",
    "    G = np.array([[1.0], [-b]])\n",
    "    H = np.array([[1.0, 0.0]])\n",
    "    dF = np.array([\n",
    "        [[1, 0], [0, 0]],  # First parameter (AR coefficient)\n",
    "        [[0, 0], [0, 0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dF.reshape(2, 2, 2)\n",
    "    dG = np.array([\n",
    "        [[0], [0]],  # First parameter (AR coefficient)\n",
    "        [[0], [-1]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dG.reshape(2, 2, 1)\n",
    "\n",
    "    g = np.array([1.0, a - b, a * (a - b)])\n",
    "\n",
    "    C = np.array([\n",
    "        sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2),\n",
    "        a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b,\n",
    "        a * (a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b)\n",
    "    ])\n",
    "\n",
    "    V = np.array([\n",
    "        [C[0], -b * g[0]],\n",
    "        [-b * g[0], b**2 * sigma2]\n",
    "    ])\n",
    "\n",
    "    dV = np.array([\n",
    "        [[(2 * sigma2 * (a-b) * (1 - a*b)) / (1 - a**2)**2, 0],\n",
    "        [0, 0]],\n",
    "        [[2 * sigma2 * (b-a) / (1 - a**2), -1],\n",
    "        [-1, 2 * b * sigma2]]\n",
    "    ])\n",
    "\n",
    "    # Initialize the x and stored values\n",
    "    x = np.zeros((k, 1))\n",
    "    dx = np.zeros((2, 2, 1))\n",
    "\n",
    "    # test\n",
    "    dict = {\n",
    "        'K1': [],\n",
    "        'dK1': [],\n",
    "        'K2': [],\n",
    "        'dK2': [],\n",
    "        'x1': [],\n",
    "        'x2': [],\n",
    "        'dx1': [],\n",
    "        'dx2': [],\n",
    "        'V11': [],\n",
    "        'V12': [],\n",
    "        'V21': [],\n",
    "        'V22': [],\n",
    "        'dV11': [],\n",
    "        'dV12': [],\n",
    "        'dV21': [],\n",
    "        'dV22': []\n",
    "    }\n",
    "\n",
    "    # \n",
    "    for t in range(N):\n",
    "        # 1. Predict\n",
    "        # Predict one-step-ahead state predictive density of x_{t}\n",
    "        x_predict = F @ x\n",
    "        V_predict = F @ V @ F.T + G @ G.T\n",
    "\n",
    "        # Compute forecast error and one-step-ahead predictive variance\n",
    "        e_t = y[t] - (H @ x_predict)[0, 0]\n",
    "        r_t = (H @ V_predict @ H.T)[0, 0]\n",
    "\n",
    "        GdGT = np.array([G @ dG.T[0][i].reshape(1,2) for i in range(2)])\n",
    "\n",
    "        # Kalman filter for gradient\n",
    "        dx_predict = F @ dx + dF @ x\n",
    "        dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + GdGT\n",
    "\n",
    "        # Calculate de_t and dr_t as tensor(2,1,1)\n",
    "        de_t = -H @ dx_predict\n",
    "        dr_t = H @ dV_predict @ H.T\n",
    "\n",
    "\n",
    "        # 2. Update\n",
    "        # Kalman gain\n",
    "        K = V_predict @ H.T / r_t\n",
    "\n",
    "        # Update current state and covariance\n",
    "        x = x_predict + K * e_t\n",
    "        V = (np.eye(k) - K @ H) @ V_predict\n",
    "\n",
    "        dK = (dV_predict @ H.T / r_t) - (V_predict @ H.T / r_t**2) @ dr_t\n",
    "        dx = dx_predict + K @ de_t + dK * e_t\n",
    "        dV = dV_predict - dK @ H @ V_predict - K @ H @ dV_predict\n",
    "        \n",
    "        # Append \n",
    "        dict['K1'].append(K[0, 0])\n",
    "        dict['dK1'].append(dK[0, 0, 0])\n",
    "        dict['K2'].append(K[1, 0])\n",
    "        dict['dK2'].append(dK[0, 1, 0])\n",
    "        dict['x1'].append(x[0, 0])\n",
    "        dict['x2'].append(x[1, 0])\n",
    "        dict['dx1'].append(dx[0, 0, 0])\n",
    "        dict['dx2'].append(dx[0, 1, 0])\n",
    "        dict['V11'].append(V[0, 0])\n",
    "        dict['V12'].append(V[0, 1])\n",
    "        dict['V21'].append(V[1, 0])\n",
    "        dict['V22'].append(V[1, 1])\n",
    "        dict['dV11'].append(dV[0, 0, 0])\n",
    "        dict['dV12'].append(dV[0, 0, 1])\n",
    "        dict['dV21'].append(dV[0, 1, 0])\n",
    "        dict['dV22'].append(dV[0, 1, 1])\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n",
    "# Compare analytical and autograd gradient\n",
    "a_test = 0.1\n",
    "b_test = 0.1\n",
    "\n",
    "# Analytical gradient\n",
    "analytical_grad = a_grad(a_test, b_test)\n",
    "\n",
    "for t in range(6):\n",
    "    grad_func = lambda a: a_grad(a, b=b_test)['x2'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(a_test)\n",
    "\n",
    "    print(f\"Autograd gradient t={t}:\", result)\n",
    "    print(f\"Analytical gradient t={t}:\", analytical_grad['dx2'][t])\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Auto and Analytical Gradient for a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dK1</th>\n",
       "      <th>dK2</th>\n",
       "      <th>dx1</th>\n",
       "      <th>dx2</th>\n",
       "      <th>dV11</th>\n",
       "      <th>dV12</th>\n",
       "      <th>dV21</th>\n",
       "      <th>dV22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dK1  dK2  dx1  dx2 dV11 dV12 dV21 dV22\n",
       "0   0.0 -0.0 -0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1   0.0 -0.0  0.0 -0.0 -0.0  0.0 -0.0  0.0\n",
       "2   0.0 -0.0  0.0  0.0  0.0  0.0  0.0 -0.0\n",
       "3   0.0 -0.0 -0.0  0.0 -0.0  0.0  0.0  0.0\n",
       "4   0.0 -0.0 -0.0  0.0  0.0  0.0  0.0 -0.0\n",
       "5  -0.0 -0.0  0.0 -0.0  0.0  0.0  0.0  0.0\n",
       "6  -0.0  0.0 -0.0  0.0  0.0 -0.0  0.0  0.0\n",
       "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0 -0.0\n",
       "8   0.0  0.0  0.0  0.0  0.0  0.0  0.0 -0.0\n",
       "9   0.0  0.0 -0.0  0.0  0.0  0.0  0.0 -0.0\n",
       "10  0.0 -0.0  0.0 -0.0  0.0  0.0  0.0 -0.0\n",
       "11  0.0 -0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "12  0.0  0.0  0.0 -0.0  0.0  0.0  0.0  0.0\n",
       "13  0.0 -0.0  0.0  0.0  0.0  0.0  0.0 -0.0\n",
       "14  0.0 -0.0  0.0  0.0  0.0  0.0  0.0 -0.0\n",
       "15  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "16  0.0 -0.0  0.0 -0.0  0.0  0.0  0.0  0.0\n",
       "17  0.0 -0.0  0.0 -0.0  0.0  0.0  0.0 -0.0\n",
       "18  0.0  0.0  0.0 -0.0  0.0  0.0  0.0  0.0\n",
       "19  0.0 -0.0  0.0  0.0  0.0  0.0  0.0 -0.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def a_grad(a, b, N):\n",
    "    k = 2\n",
    "    sigma2 = 100\n",
    "\n",
    "    # Simple operations that autograd can handle\n",
    "    F = np.array([[a, 1.0], [0.0, 0.0]])\n",
    "    G = np.array([[1.0], [-b]])\n",
    "    H = np.array([[1.0, 0.0]])\n",
    "    dF = np.array([\n",
    "        [[1, 0], [0, 0]],  # First parameter (AR coefficient)\n",
    "        [[0, 0], [0, 0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dF.reshape(2, 2, 2)\n",
    "    dG = np.array([\n",
    "        [[0], [0]],  # First parameter (AR coefficient)\n",
    "        [[0], [-1]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dG.reshape(2, 2, 1)\n",
    "\n",
    "    g = np.array([1.0, a - b, a * (a - b)])\n",
    "\n",
    "    C = np.array([\n",
    "        sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2),\n",
    "        a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b,\n",
    "        a * (a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b)\n",
    "    ])\n",
    "\n",
    "    V = np.array([\n",
    "        [C[0], -b * g[0]],\n",
    "        [-b * g[0], b**2 * sigma2]\n",
    "    ])\n",
    "\n",
    "    dV = np.array([\n",
    "        [[(2 * sigma2 * (a-b) * (1 - a*b)) / (1 - a**2)**2, 0],\n",
    "        [0, 0]],\n",
    "        [[2 * sigma2 * (b-a) / (1 - a**2), -1],\n",
    "        [-1, 2 * b * sigma2]]\n",
    "    ])\n",
    "\n",
    "    # Initialize the x and dx\n",
    "    x = np.zeros((k, 1))\n",
    "    dx = np.zeros((2, 2, 1))\n",
    "\n",
    "    # Store the value into dictionary\n",
    "    dict = {\n",
    "        'K1': [],\n",
    "        'dK1': [],\n",
    "        'K2': [],\n",
    "        'dK2': [],\n",
    "        'x1': [],\n",
    "        'x2': [],\n",
    "        'dx1': [],\n",
    "        'dx2': [],\n",
    "        'V11': [],\n",
    "        'V12': [],\n",
    "        'V21': [],\n",
    "        'V22': [],\n",
    "        'dV11': [],\n",
    "        'dV12': [],\n",
    "        'dV21': [],\n",
    "        'dV22': []\n",
    "    }\n",
    "\n",
    "    # \n",
    "    for t in range(N):\n",
    "        # 1. Predict\n",
    "        # Predict one-step-ahead state predictive density of x_{t}\n",
    "        x_predict = F @ x\n",
    "        V_predict = F @ V @ F.T + G @ G.T\n",
    "\n",
    "        # Compute forecast error and one-step-ahead predictive variance\n",
    "        e_t = y[t] - (H @ x_predict)[0, 0]\n",
    "        r_t = (H @ V_predict @ H.T)[0, 0]\n",
    "\n",
    "        GdGT = np.array([G @ dG.T[0][i].reshape(1,2) for i in range(2)])\n",
    "\n",
    "        # Kalman filter for gradient\n",
    "        dx_predict = F @ dx + dF @ x\n",
    "        dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + GdGT\n",
    "\n",
    "        # Calculate de_t and dr_t as tensor(2,1,1)\n",
    "        de_t = -H @ dx_predict\n",
    "        dr_t = H @ dV_predict @ H.T\n",
    "\n",
    "\n",
    "        # 2. Update\n",
    "        # Kalman gain\n",
    "        K = V_predict @ H.T / r_t\n",
    "\n",
    "        # Update current state and covariance\n",
    "        x = x_predict + K * e_t\n",
    "        V = (np.eye(k) - K @ H) @ V_predict\n",
    "\n",
    "        dK = (dV_predict @ H.T / r_t) - (V_predict @ H.T / r_t**2) @ dr_t\n",
    "        dx = dx_predict + K @ de_t + dK * e_t\n",
    "        dV = dV_predict - dK @ H @ V_predict - K @ H @ dV_predict\n",
    "        \n",
    "        # Append \n",
    "        dict['K1'].append(K[0, 0])\n",
    "        dict['dK1'].append(dK[0, 0, 0]) # Analytic gradient\n",
    "        dict['K2'].append(K[1, 0])\n",
    "        dict['dK2'].append(dK[0, 1, 0])\n",
    "        dict['x1'].append(x[0, 0])\n",
    "        dict['x2'].append(x[1, 0])\n",
    "        dict['dx1'].append(dx[0, 0, 0])\n",
    "        dict['dx2'].append(dx[0, 1, 0])\n",
    "        dict['V11'].append(V[0, 0])\n",
    "        dict['V12'].append(V[0, 1])\n",
    "        dict['V21'].append(V[1, 0])\n",
    "        dict['V22'].append(V[1, 1])\n",
    "        dict['dV11'].append(dV[0, 0, 0])\n",
    "        dict['dV12'].append(dV[0, 0, 1])\n",
    "        dict['dV21'].append(dV[0, 1, 0])\n",
    "        dict['dV22'].append(dV[0, 1, 1])\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n",
    "# Compare analytical and autograd gradient\n",
    "a_test = 0.1\n",
    "b_test = 0.2\n",
    "\n",
    "# Dataframe to store the differences\n",
    "df = pd.DataFrame(columns=['dK1', 'dK2', 'dx1', 'dx2', 'dV11', 'dV12', 'dV21', 'dV22'])\n",
    "\n",
    "# Analytical gradient\n",
    "analytical_grad = a_grad(a_test, b_test, N=20)\n",
    "\n",
    "# threshold = 1e-5\n",
    "roundup = 6\n",
    "\n",
    "for t in range(20):\n",
    "\n",
    "    # 1. Difference between dK1\n",
    "    grad_func = lambda a: a_grad(a, b=b_test, N=t+1)['K1'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(a_test)\n",
    "    df.loc[t, 'dK1'] = round(result - analytical_grad['dK1'][t], roundup)\n",
    "\n",
    "    # 2. Difference between dK2\n",
    "    grad_func = lambda a: a_grad(a, b=b_test, N=t+1)['K2'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(a_test)\n",
    "    df.loc[t, 'dK2'] = round(result - analytical_grad['dK2'][t], roundup)\n",
    "\n",
    "    # 3. Difference between dx1\n",
    "    grad_func = lambda a: a_grad(a, b=b_test, N=t+1)['x1'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(a_test)\n",
    "    df.loc[t, 'dx1'] = round(result - analytical_grad['dx1'][t], roundup)\n",
    "\n",
    "    # 4. Difference between dx2\n",
    "    grad_func = lambda a: a_grad(a, b=b_test, N=t+1)['x2'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(a_test)\n",
    "    df.loc[t, 'dx2'] = round(result - analytical_grad['dx2'][t], roundup)\n",
    "\n",
    "    # 5. Difference between dV11\n",
    "    grad_func = lambda a: a_grad(a, b=b_test, N=t+1)['V11'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(a_test)\n",
    "    df.loc[t, 'dV11'] = round(result - analytical_grad['dV11'][t], roundup)\n",
    "\n",
    "    # 6. Difference between dV12\n",
    "    grad_func = lambda a: a_grad(a, b=b_test, N=t+1)['V12'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(a_test)\n",
    "    df.loc[t, 'dV12'] = round(result - analytical_grad['dV12'][t], roundup)\n",
    "\n",
    "    # 7. Difference between dV21\n",
    "    grad_func = lambda a: a_grad(a, b=b_test, N=t+1)['V21'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(a_test)\n",
    "    df.loc[t, 'dV21'] = round(result - analytical_grad['dV21'][t], roundup)\n",
    "\n",
    "    # 8. Difference between dV22\n",
    "    grad_func = lambda a: a_grad(a, b=b_test, N=t+1)['V22'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(a_test)\n",
    "    df.loc[t, 'dV22'] = round(result - analytical_grad['dV22'][t], roundup)\n",
    "\n",
    "\n",
    "# Print result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Auto and Analytical Gradient for b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dK1</th>\n",
       "      <th>dK2</th>\n",
       "      <th>dx1</th>\n",
       "      <th>dx2</th>\n",
       "      <th>dV11</th>\n",
       "      <th>dV12</th>\n",
       "      <th>dV21</th>\n",
       "      <th>dV22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dK1  dK2  dx1  dx2 dV11 dV12 dV21 dV22\n",
       "0   0.0 -0.0  0.0 -0.0 -0.0  0.0  0.0 -0.0\n",
       "1   0.0  0.0 -0.0  0.0  0.0  0.0 -0.0  0.0\n",
       "2   0.0  0.0 -0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3   0.0 -0.0 -0.0  0.0 -0.0 -0.0 -0.0  0.0\n",
       "4   0.0  0.0  0.0 -0.0  0.0  0.0 -0.0  0.0\n",
       "5  -0.0  0.0  0.0 -0.0  0.0  0.0  0.0 -0.0\n",
       "6   0.0  0.0  0.0 -0.0 -0.0  0.0  0.0 -0.0\n",
       "7   0.0 -0.0  0.0 -0.0 -0.0 -0.0  0.0  0.0\n",
       "8   0.0  0.0  0.0 -0.0  0.0  0.0 -0.0  0.0\n",
       "9   0.0  0.0  0.0 -0.0  0.0 -0.0  0.0  0.0\n",
       "10  0.0 -0.0  0.0 -0.0  0.0  0.0  0.0 -0.0\n",
       "11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "12  0.0  0.0  0.0 -0.0  0.0 -0.0  0.0  0.0\n",
       "13  0.0 -0.0  0.0  0.0  0.0  0.0  0.0 -0.0\n",
       "14  0.0 -0.0  0.0  0.0  0.0  0.0  0.0 -0.0\n",
       "15  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -0.0\n",
       "16  0.0 -0.0  0.0  0.0  0.0  0.0  0.0 -0.0\n",
       "17  0.0  0.0  0.0  0.0  0.0  0.0 -0.0  0.0\n",
       "18  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "19  0.0  0.0  0.0 -0.0  0.0  0.0 -0.0  0.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def b_grad(b, a, N):\n",
    "    k = 2\n",
    "    sigma2 = 100\n",
    "\n",
    "    # Simple operations that autograd can handle\n",
    "    F = np.array([[a, 1.0], [0.0, 0.0]])\n",
    "    G = np.array([[1.0], [-b]])\n",
    "    H = np.array([[1.0, 0.0]])\n",
    "    dF = np.array([\n",
    "        [[1, 0], [0, 0]],  # First parameter (AR coefficient)\n",
    "        [[0, 0], [0, 0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dF.reshape(2, 2, 2)\n",
    "    dG = np.array([\n",
    "        [[0], [0]],  # First parameter (AR coefficient)\n",
    "        [[0], [-1]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dG.reshape(2, 2, 1)\n",
    "\n",
    "    g = np.array([1.0, a - b, a * (a - b)])\n",
    "\n",
    "    C = np.array([\n",
    "        sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2),\n",
    "        a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b,\n",
    "        a * (a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b)\n",
    "    ])\n",
    "\n",
    "    V = np.array([\n",
    "        [C[0], -b * g[0]],\n",
    "        [-b * g[0], b**2 * sigma2]\n",
    "    ])\n",
    "\n",
    "    dV = np.array([\n",
    "        [[(2 * sigma2 * (a-b) * (1 - a*b)) / (1 - a**2)**2, 0],\n",
    "        [0, 0]],\n",
    "        [[2 * sigma2 * (b-a) / (1 - a**2), -1],\n",
    "        [-1, 2 * b * sigma2]]\n",
    "    ])\n",
    "\n",
    "    # Initialize the x and dx\n",
    "    x = np.zeros((k, 1))\n",
    "    dx = np.zeros((2, 2, 1))\n",
    "\n",
    "    # Store the value into dictionary\n",
    "    dict = {\n",
    "        'K1': [],\n",
    "        'dK1': [],\n",
    "        'K2': [],\n",
    "        'dK2': [],\n",
    "        'x1': [],\n",
    "        'x2': [],\n",
    "        'dx1': [],\n",
    "        'dx2': [],\n",
    "        'V11': [],\n",
    "        'V12': [],\n",
    "        'V21': [],\n",
    "        'V22': [],\n",
    "        'dV11': [],\n",
    "        'dV12': [],\n",
    "        'dV21': [],\n",
    "        'dV22': []\n",
    "    }\n",
    "\n",
    "    # \n",
    "    for t in range(N):\n",
    "        # 1. Predict\n",
    "        # Predict one-step-ahead state predictive density of x_{t}\n",
    "        x_predict = F @ x\n",
    "        V_predict = F @ V @ F.T + G @ G.T\n",
    "\n",
    "        # Compute forecast error and one-step-ahead predictive variance\n",
    "        e_t = y[t] - (H @ x_predict)[0, 0]\n",
    "        r_t = (H @ V_predict @ H.T)[0, 0]\n",
    "\n",
    "        GdGT = np.array([G @ dG.T[0][i].reshape(1,2) for i in range(2)])\n",
    "\n",
    "        # Kalman filter for gradient\n",
    "        dx_predict = F @ dx + dF @ x\n",
    "        dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + GdGT\n",
    "\n",
    "        # Calculate de_t and dr_t as tensor(2,1,1)\n",
    "        de_t = -H @ dx_predict\n",
    "        dr_t = H @ dV_predict @ H.T\n",
    "\n",
    "\n",
    "        # 2. Update\n",
    "        # Kalman gain\n",
    "        K = V_predict @ H.T / r_t\n",
    "\n",
    "        # Update current state and covariance\n",
    "        x = x_predict + K * e_t\n",
    "        V = (np.eye(k) - K @ H) @ V_predict\n",
    "\n",
    "        dK = (dV_predict @ H.T / r_t) - (V_predict @ H.T / r_t**2) @ dr_t\n",
    "        dx = dx_predict + K @ de_t + dK * e_t\n",
    "        dV = dV_predict - dK @ H @ V_predict - K @ H @ dV_predict\n",
    "        \n",
    "        # Append \n",
    "        dict['K1'].append(K[0, 0])\n",
    "        dict['dK1'].append(dK[1, 0, 0]) # Analytic gradient\n",
    "        dict['K2'].append(K[1, 0])\n",
    "        dict['dK2'].append(dK[1, 1, 0])\n",
    "        dict['x1'].append(x[0, 0])\n",
    "        dict['x2'].append(x[1, 0])\n",
    "        dict['dx1'].append(dx[1, 0, 0])\n",
    "        dict['dx2'].append(dx[1, 1, 0])\n",
    "        dict['V11'].append(V[0, 0])\n",
    "        dict['V12'].append(V[0, 1])\n",
    "        dict['V21'].append(V[1, 0])\n",
    "        dict['V22'].append(V[1, 1])\n",
    "        dict['dV11'].append(dV[1, 0, 0])\n",
    "        dict['dV12'].append(dV[1, 0, 1])\n",
    "        dict['dV21'].append(dV[1, 1, 0])\n",
    "        dict['dV22'].append(dV[1, 1, 1])\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n",
    "# Compare analytical and autograd gradient\n",
    "a_test = -0.1\n",
    "b_test = 0.3\n",
    "\n",
    "# Dataframe to store the differences\n",
    "df = pd.DataFrame(columns=['dK1', 'dK2', 'dx1', 'dx2', 'dV11', 'dV12', 'dV21', 'dV22'])\n",
    "\n",
    "# Analytical gradient\n",
    "analytical_grad = b_grad(b_test, a_test, N=20)\n",
    "\n",
    "# threshold = 1e-5\n",
    "roundup = 6\n",
    "\n",
    "for t in range(20):\n",
    "\n",
    "    # 1. Difference between dK1\n",
    "    grad_func = lambda b: b_grad(b, a=a_test, N=t+1)['K1'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(b_test)\n",
    "    df.loc[t, 'dK1'] = round(result - analytical_grad['dK1'][t], roundup)\n",
    "\n",
    "    # 2. Difference between dK2\n",
    "    grad_func = lambda b: b_grad(b, a=a_test, N=t+1)['K2'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(b_test)\n",
    "    df.loc[t, 'dK2'] = round(result - analytical_grad['dK2'][t], roundup)\n",
    "\n",
    "    # 3. Difference between dx1\n",
    "    grad_func = lambda b: b_grad(b, a=a_test, N=t+1)['x1'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(b_test)\n",
    "    df.loc[t, 'dx1'] = round(result - analytical_grad['dx1'][t], roundup)\n",
    "\n",
    "    # 4. Difference between dx2\n",
    "    grad_func = lambda b: b_grad(b, a=a_test, N=t+1)['x2'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(b_test)\n",
    "    df.loc[t, 'dx2'] = round(result - analytical_grad['dx2'][t], roundup)\n",
    "\n",
    "    # 5. Difference between dV11\n",
    "    grad_func = lambda b: b_grad(b, a=a_test, N=t+1)['V11'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(b_test)\n",
    "    df.loc[t, 'dV11'] = round(result - analytical_grad['dV11'][t], roundup)\n",
    "\n",
    "    # 6. Difference between dV12\n",
    "    grad_func = lambda b: b_grad(b, a=a_test, N=t+1)['V12'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(b_test)\n",
    "    df.loc[t, 'dV12'] = round(result - analytical_grad['dV12'][t], roundup)\n",
    "\n",
    "    # 7. Difference between dV21\n",
    "    grad_func = lambda b: b_grad(b, a=a_test, N=t+1)['V21'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(b_test)\n",
    "    df.loc[t, 'dV21'] = round(result - analytical_grad['dV21'][t], roundup)\n",
    "\n",
    "    # 8. Difference between dV22\n",
    "    grad_func = lambda b: b_grad(b, a=a_test, N=t+1)['V22'][t]\n",
    "    auto_grad = grad(grad_func)\n",
    "    result = auto_grad(b_test)\n",
    "    df.loc[t, 'dV22'] = round(result - analytical_grad['dV22'][t], roundup)\n",
    "\n",
    "\n",
    "# Print result\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
