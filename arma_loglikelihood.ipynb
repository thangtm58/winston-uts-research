{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e22fff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import autograd.scipy.stats as sps_autograd\n",
    "from autograd import grad, hessian\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import toeplitz\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043fc02f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m theta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([a, b])\n\u001b[1;32m     20\u001b[0m init_sigma2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 21\u001b[0m obj, sigma2_hat \u001b[38;5;241m=\u001b[39m \u001b[43mauto_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_sigma2\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     23\u001b[0m np\u001b[38;5;241m.\u001b[39msqrt(sigma2_hat)\n",
      "Cell \u001b[0;32mIn[119], line 74\u001b[0m, in \u001b[0;36mauto_grad\u001b[0;34m(theta, init_sigma2)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Kalman filter for gradient\u001b[39;00m\n\u001b[1;32m     73\u001b[0m dx_predict \u001b[38;5;241m=\u001b[39m F \u001b[38;5;241m@\u001b[39m dx \u001b[38;5;241m+\u001b[39m dF \u001b[38;5;241m@\u001b[39m x\n\u001b[0;32m---> 74\u001b[0m dV_predict \u001b[38;5;241m=\u001b[39m F \u001b[38;5;241m@\u001b[39m dV \u001b[38;5;241m@\u001b[39m F\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m dF \u001b[38;5;241m@\u001b[39m V \u001b[38;5;241m@\u001b[39m F\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m F \u001b[38;5;241m@\u001b[39m V \u001b[38;5;241m@\u001b[39m dF\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m \u001b[43mdG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m \u001b[38;5;241m@\u001b[39m G\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m GdGT\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Calculate de_t and dr_t as tensor(2,1,1)\u001b[39;00m\n\u001b[1;32m     77\u001b[0m de_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mH \u001b[38;5;241m@\u001b[39m dx_predict\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simulate ARMA(1, 1) model\n",
    "\"\"\"\n",
    "a = 0.5\n",
    "b = 0.2\n",
    "# Define AR and MA coefficients\n",
    "ar = np.array([1, -a])  \n",
    "ma = np.array([1, -b])        \n",
    "\n",
    "# Create ARMA process object\n",
    "arma_process = ArmaProcess(ar, ma)\n",
    "\n",
    "# Simulate 500 samples\n",
    "N = 10000\n",
    "y = arma_process.generate_sample(nsample=N, scale=2) # scale is the variance of the white noise\n",
    "\n",
    "\n",
    "# Initialize the parameters\n",
    "theta = np.array([a, b])\n",
    "init_sigma2 = 100\n",
    "obj, sigma2_hat = auto_grad(theta, init_sigma2)  \n",
    "\n",
    "np.sqrt(sigma2_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_grad(theta, init_sigma2):\n",
    "    a, b = theta\n",
    "    k = 2\n",
    "    sigma2 = init_sigma2  # CHANGE PARAMETER HERE\n",
    "\n",
    "    # Simple operations that autograd can handle\n",
    "    F = np.array([[a, 1.0], [0.0, 0.0]])\n",
    "    G = np.array([[1.0], [-b]])\n",
    "    H = np.array([[1.0, 0.0]])\n",
    "    # Q = np.eye(2)\n",
    "    dF = np.array([\n",
    "        [[1, 0], [0, 0]],  # First parameter (AR coefficient)\n",
    "        [[0, 0], [0, 0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dF.reshape(2, 2, 2)\n",
    "    dG = np.array([\n",
    "        [[0], [0]],  # First parameter (AR coefficient)\n",
    "        [[0], [-1.0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dG.reshape(2, 2, 1)\n",
    "\n",
    "    g = np.array([1.0, a - b, a * (a - b)])\n",
    "\n",
    "    C = np.array([\n",
    "        sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2),\n",
    "        a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b,\n",
    "        a * (a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b)\n",
    "    ])\n",
    "\n",
    "    V = np.array([\n",
    "        [C[0], -b * g[0]],\n",
    "        [-b * g[0], b**2 * sigma2]\n",
    "    ])\n",
    "\n",
    "    dV = np.array([\n",
    "        [[(2 * sigma2 * (a-b) * (1 - a*b)) / (1 - a**2)**2, 0],\n",
    "        [0, 0]],\n",
    "        [[2 * sigma2 * (b-a) / (1 - a**2), -1],\n",
    "        [-1, 2 * b * sigma2]]\n",
    "    ])\n",
    "\n",
    "    # Initialize the x and dx\n",
    "    x = np.zeros((k, 1))\n",
    "    dx = np.zeros((2, 2, 1))\n",
    "\n",
    "    # Dictionary to store the values\n",
    "    dict = {\n",
    "        'analytical_grad': [],\n",
    "        'log_likelihood': []\n",
    "    }\n",
    "\n",
    "    sigma2_hat_sum = 0.0\n",
    "    dsigma2_hat_sum = np.array([[0.0], [0.0]]).reshape(2, 1, 1)\n",
    "    # \n",
    "    for t in range(N):\n",
    "        # Set the current sample size\n",
    "        n = t + 1\n",
    "        \n",
    "\n",
    "        # 1. Predict\n",
    "        # Predict one-step-ahead state predictive density of x_{t}\n",
    "        x_predict = F @ x\n",
    "        V_predict = F @ V @ F.T + G @ G.T\n",
    "\n",
    "        # Compute forecast error and one-step-ahead predictive variance\n",
    "        e_t = y[t] - (H @ x_predict)[0, 0]\n",
    "        r_t = (H @ V_predict @ H.T)[0, 0]\n",
    "\n",
    "\n",
    "        GdGT = np.array([G @ dG.T[0][i].reshape(1,2) for i in range(2)])\n",
    "\n",
    "        # Kalman filter for gradient\n",
    "        dx_predict = F @ dx + dF @ x\n",
    "        dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + GdGT\n",
    "\n",
    "        # Calculate de_t and dr_t as tensor(2,1,1)\n",
    "        de_t = -H @ dx_predict\n",
    "        dr_t = H @ dV_predict @ H.T\n",
    "\n",
    "        # Update sigma2 hat and gradient of sigma2 hat\n",
    "        sigma2_hat_sum += e_t**2 / r_t\n",
    "        sigma2_hat = sigma2_hat_sum / n\n",
    "        dsigma2_hat_sum += (2 * e_t * de_t) /r_t - (e_t**2 * dr_t) / (r_t**2)\n",
    "        dsigma2_hat = dsigma2_hat_sum / n\n",
    "\n",
    "\n",
    "        # 2. Update\n",
    "        # Kalman gain\n",
    "        K = V_predict @ H.T / r_t\n",
    "\n",
    "        # Update current state and covariance\n",
    "        x = x_predict + K * e_t\n",
    "        V = (np.eye(k) - K @ H) @ V_predict\n",
    "\n",
    "        dK = (dV_predict @ H.T / r_t) - (V_predict @ H.T / r_t**2) @ dr_t\n",
    "        dx = dx_predict + K @ de_t + dK * e_t\n",
    "        dV = dV_predict - dK @ H @ V_predict - K @ H @ dV_predict\n",
    "        \n",
    "        # Compute sigma2_hat and gradient of the log-likelihood\n",
    "        log_likelihood = -0.5 * (np.log(2 * np.pi) \n",
    "                                + np.log(sigma2_hat) \n",
    "                                + np.log(r_t) + e_t**2 / (r_t * sigma2_hat))  \n",
    "        \n",
    "        \"\"\"\n",
    "        analytical_grad = - (e_t * de_t) / (sigma2_hat * r_t) \\\n",
    "                                + (e_t**2 * dr_t) / (2 * sigma2_hat * r_t**2) \\\n",
    "                                - dr_t / (2 * r_t)\n",
    "        \"\"\"\n",
    "        \n",
    "        analytical_grad = - 0.5 * (dsigma2_hat / sigma2_hat \n",
    "                                    + dr_t / r_t \n",
    "                                    + (2 * e_t * sigma2_hat * r_t * de_t - e_t**2 * r_t * dsigma2_hat - e_t**2 * sigma2_hat * dr_t) / (sigma2_hat**2 * r_t**2)\n",
    "                                    )\n",
    "        \n",
    "\n",
    "        dict['analytical_grad'].append(analytical_grad.flatten())\n",
    "        dict['log_likelihood'].append(log_likelihood)\n",
    "\n",
    "    # Return the dictionary\n",
    "    return dict, sigma2_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2189e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>analytical_grad</th>\n",
       "      <th>autograd</th>\n",
       "      <th>equal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>[0.49507189378486227, -0.5522801086596363]</td>\n",
       "      <td>[0.49507189378486244, -0.5522801086596362]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>[0.5969777415824398, -0.6797712832811074]</td>\n",
       "      <td>[0.5969777415824399, -0.6797712832811076]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>[0.0995772364982111, -0.16071296360609078]</td>\n",
       "      <td>[0.09957723649821103, -0.16071296360609078]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>[0.00413524151394562, -0.06638362821102806]</td>\n",
       "      <td>[0.004135241513945483, -0.06638362821102811]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>[-0.9431008807668196, 0.6206769294591853]</td>\n",
       "      <td>[-0.9431008807668194, 0.6206769294591852]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     t                              analytical_grad  \\\n",
       "95  95   [0.49507189378486227, -0.5522801086596363]   \n",
       "96  96    [0.5969777415824398, -0.6797712832811074]   \n",
       "97  97   [0.0995772364982111, -0.16071296360609078]   \n",
       "98  98  [0.00413524151394562, -0.06638362821102806]   \n",
       "99  99    [-0.9431008807668196, 0.6206769294591853]   \n",
       "\n",
       "                                        autograd equal  \n",
       "95    [0.49507189378486244, -0.5522801086596362]  True  \n",
       "96     [0.5969777415824399, -0.6797712832811076]  True  \n",
       "97   [0.09957723649821103, -0.16071296360609078]  True  \n",
       "98  [0.004135241513945483, -0.06638362821102811]  True  \n",
       "99     [-0.9431008807668194, 0.6206769294591852]  True  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the parameters\n",
    "theta = np.array([-0.2, 0.1])\n",
    "init_sigma2 = 0.5\n",
    "obj, sigma2_hat = auto_grad(theta, init_sigma2)  \n",
    "\n",
    "# Dataframe to store the results\n",
    "df = pd.DataFrame(columns=['t', 'analytical_grad', 'autograd', 'equal'])\n",
    "\n",
    "for t in range(N):\n",
    "    # Analytical gradient\n",
    "    analytical_grad = obj['analytical_grad'][t]\n",
    "\n",
    "    # Autograd\n",
    "    obj_func_likelihood = lambda param: auto_grad(param, init_sigma2)[0]['log_likelihood'][t]\n",
    "    grad_obj_func_likelihood = grad(obj_func_likelihood)\n",
    "    auto_grad_val = grad_obj_func_likelihood(theta)\n",
    "\n",
    "    # Store the results\n",
    "    df.loc[t, 't'] = t\n",
    "    df.loc[t, 'analytical_grad'] = analytical_grad\n",
    "    df.loc[t, 'autograd'] = auto_grad_val\n",
    "    df.loc[t, 'equal'] = np.allclose(analytical_grad, auto_grad_val)\n",
    "\n",
    "# Check the results\n",
    "print(sum(df['equal']) == N)\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b48915e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.996055280378177)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the parameters\n",
    "theta = np.array([a, b])\n",
    "init_sigma2 = 1\n",
    "obj, sigma2_hat = auto_grad(theta, init_sigma2)  \n",
    "\n",
    "np.sqrt(sigma2_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "817bc083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
