{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb5f60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of PyMC you are using is very outdated.\n",
      "\n",
      "Please upgrade to the latest version of PyMC https://www.pymc.io/projects/docs/en/stable/installation.html\n",
      "\n",
      "Also notice that PyMC3 has been renamed to PyMC.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib, time, copy\n",
    "import autograd.numpy as np\n",
    "import autograd.scipy.stats as sps_autograd\n",
    "from autograd import grad, hessian, jacobian\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import toeplitz\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "import matplotlib.pyplot as plt \n",
    "from pymc3 import ess as effective_sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc1d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simulate ARMA(1, 1) model\n",
    "\"\"\"\n",
    "theta0 = np.array([0.5, 0.2]) # True parameter values\n",
    "\n",
    "# Define AR and MA coefficients\n",
    "ar = np.array([1, -theta0[0]])  \n",
    "ma = np.array([1, -theta0[1]])        \n",
    "\n",
    "# Create ARMA process object\n",
    "arma_process = ArmaProcess(ar, ma)\n",
    "\n",
    "# Simulate 500 samples\n",
    "N = 200\n",
    "y = arma_process.generate_sample(nsample=N, scale=2) # scale is the variance of the white noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1fda0",
   "metadata": {},
   "source": [
    "# Define function for the whole Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558bb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_KF_parameter_values(theta, sigma2):\n",
    "    \"\"\"\n",
    "    Initialise the parameters for the Kalman Filter\n",
    "    \"\"\"\n",
    "    # region Initial values for Kalman Filter\n",
    "    a, b = theta\n",
    "    k = len(theta)\n",
    "\n",
    "    # Simple operations that autograd can handle\n",
    "    F = np.array([[a, 1.0], [0.0, 0.0]])\n",
    "    G = np.array([[1.0], [-b]])\n",
    "    H = np.array([[1.0, 0.0]])\n",
    "    Q = np.eye(k)\n",
    "    dF = np.array([\n",
    "        [[1, 0], [0, 0]],  # First parameter (AR coefficient)\n",
    "        [[0, 0], [0, 0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dF.reshape(2, 2, 2)\n",
    "    dG = np.array([\n",
    "        [[0], [0]],  # First parameter (AR coefficient)\n",
    "        [[0], [-1.0]]   # Second parameter (MA coefficient)\n",
    "        ])\n",
    "    dG.reshape(2, 2, 1)\n",
    "\n",
    "    g = np.array([1.0, a - b, a * (a - b)])\n",
    "\n",
    "    C = np.array([\n",
    "        sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2),\n",
    "        a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b,\n",
    "        a * (a * (sigma2 * (1 - 2 * a * b + b**2) / (1 - a**2)) - sigma2 * b)\n",
    "    ])\n",
    "\n",
    "    V = np.array([\n",
    "        [C[0], -b * g[0]],\n",
    "        [-b * g[0], b**2 * sigma2]\n",
    "    ])\n",
    "\n",
    "    dV = np.array([\n",
    "        [[(2 * sigma2 * (a-b) * (1 - a*b)) / (1 - a**2)**2, 0],\n",
    "        [0, 0]],\n",
    "        [[2 * sigma2 * (b-a) / (1 - a**2), -1],\n",
    "        [-1, 2 * b * sigma2]]\n",
    "    ])\n",
    "\n",
    "    # Initialize the x and dx\n",
    "    x = np.zeros((k, 1))\n",
    "    dx = np.zeros((2, 2, 1))\n",
    "\n",
    "    return F, G, H, dF, dG, V, dV, x, dx, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2edfaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_grad_hess(theta, init_sigma2, y):\n",
    "\n",
    "    N = len(y)\n",
    "\n",
    "    # Dictionary to store the values\n",
    "    dict = {\n",
    "        'analytical_grad_at_i': [],\n",
    "        'log_likelihood_at_i': [],\n",
    "    }\n",
    "\n",
    "    # Initialize the values\n",
    "    F, G, H, dF, dG, V, dV, x, dx, k = initiate_KF_parameter_values(theta, init_sigma2)\n",
    "    sigma2_hat_sum = 0.0\n",
    "    dsigma2_hat_sum = np.array([[0.0], [0.0]]).reshape(2, 1, 1)\n",
    "    log_r_sum = 0.0\n",
    "\n",
    "\n",
    "    # Run the Kalman filter for gradient\n",
    "    for t in range(N):\n",
    "        # Set the current sample size\n",
    "        n = t + 1\n",
    "    \n",
    "        # 1. Predict\n",
    "        # Predict one-step-ahead state predictive density of x_{t}\n",
    "        x_predict = F @ x\n",
    "        V_predict = F @ V @ F.T + G @ G.T\n",
    "\n",
    "        # Compute forecast error and one-step-ahead predictive variance\n",
    "        e_t = y[t] - (H @ x_predict)[0, 0]\n",
    "        r_t = (H @ V_predict @ H.T)[0, 0]\n",
    "\n",
    "        GdGT = np.array([G @ dG.T[0][i].reshape(1,2) for i in range(2)])\n",
    "\n",
    "        # Kalman filter for gradient\n",
    "        dx_predict = F @ dx + dF @ x\n",
    "        dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + GdGT\n",
    "        # dV_predict = F @ dV @ F.T + dF @ V @ F.T + F @ V @ dF.T + dG @ G.T + G @ Q @ dG.T\n",
    "\n",
    "\n",
    "        # Calculate de_t and dr_t as tensor(2,1,1)\n",
    "        de_t = -H @ dx_predict\n",
    "        dr_t = H @ dV_predict @ H.T\n",
    "\n",
    "        # Update sigma2 hat and gradient of sigma2 hat\n",
    "        sigma2_hat_sum += e_t**2 / r_t\n",
    "        sigma2_hat = sigma2_hat_sum / n\n",
    "        dsigma2_hat_sum += (2 * e_t * de_t) /r_t - (e_t**2 * dr_t) / (r_t**2)\n",
    "        dsigma2_hat = dsigma2_hat_sum / n\n",
    "\n",
    "        # Update the r\n",
    "        log_r_sum += np.log(r_t)\n",
    "\n",
    "        # 2. Update\n",
    "        # Kalman gain\n",
    "        K = V_predict @ H.T / r_t\n",
    "\n",
    "        # Update current state and covariance\n",
    "        x = x_predict + K * e_t\n",
    "        V = (np.eye(k) - K @ H) @ V_predict\n",
    "\n",
    "        dK = (dV_predict @ H.T / r_t) - (V_predict @ H.T / r_t**2) @ dr_t\n",
    "        dx = dx_predict + K @ de_t + dK * e_t\n",
    "        dV = dV_predict - dK @ H @ V_predict - K @ H @ dV_predict\n",
    "        \n",
    "        # Compute sigma2_hat and gradient of the log-likelihood\n",
    "        log_likelihood_at_i = -0.5 * (np.log(2 * np.pi) \n",
    "                                + np.log(sigma2_hat) \n",
    "                                + np.log(r_t) + e_t**2 / (r_t * sigma2_hat))  \n",
    "    \n",
    "        \n",
    "        analytical_grad_at_i = - 0.5 * (dsigma2_hat / sigma2_hat \n",
    "                                    + dr_t / r_t \n",
    "                                    + (2 * e_t * sigma2_hat * r_t * de_t - e_t**2 * r_t * dsigma2_hat - e_t**2 * sigma2_hat * dr_t) / (sigma2_hat**2 * r_t**2)\n",
    "                                    )\n",
    "        \n",
    "\n",
    "        dict['analytical_grad_at_i'].append(analytical_grad_at_i.flatten())\n",
    "        dict['log_likelihood_at_i'].append(log_likelihood_at_i)\n",
    "\n",
    "    log_likelihood = -0.5 * (N * np.log(2 * np.pi) \n",
    "                            + N * np.log(sigma2_hat) \n",
    "                            # + np.sum(np.log(r)) + N)\n",
    "                            + log_r_sum + N)\n",
    "\n",
    "    # Return the dictionary\n",
    "    return dict, sigma2_hat, log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94712e61",
   "metadata": {},
   "source": [
    "# Function to evaluate q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c252049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(theta):\n",
    "    \"\"\"\n",
    "    Simple indepent normal prior with mean 0 and variance 10^2\n",
    "    \"\"\"\n",
    "    return np.sum(sps_autograd.norm.logpdf(theta, 0, 10**2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b927b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 401.065332\n",
      "         Iterations: 9\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 15\n",
      "True parameter values\n",
      "[0.5 0.2]\n",
      "MAP estimates\n",
      "[0.68 0.37]\n"
     ]
    }
   ],
   "source": [
    "# Define the log-posterior\n",
    "log_posterior = lambda theta: analytical_grad_hess(theta, init_sigma2=1.5, y=y)[2] + log_prior(theta) \n",
    "obj_func_posterior = lambda theta: -log_posterior(theta) # For maximisation purpose\n",
    "# grad_obj_func_posterior = lambda theta: analytical_grad_hess(theta, init_sigma2=0.5)[0]['analytical_grad_at_i'][-1] # Gradient of obj_func_posterior\n",
    "# Hess_obj_func_posterior  = hessian(obj_func_posterior) # Hessian of obj_func_posterior. Computed by automatic differentiation\n",
    "\n",
    "# Find starting values by optimising the log-posterior\n",
    "theta_optim_start = np.zeros(2)\n",
    "res_optim_posterior = minimize(obj_func_posterior, theta_optim_start, method='BFGS', options={'gtol': 1e-04, 'maxiter': 1000, 'disp': True})\n",
    "\n",
    "# Compare answers\n",
    "print('True parameter values')\n",
    "print(theta0)\n",
    "print('MAP estimates')\n",
    "MAP = res_optim_posterior.x\n",
    "print(np.round(MAP, 2))\n",
    "\n",
    "# Control variate expanded around this\n",
    "thetaStar = MAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab04e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_control_variate_quantities(thetaStar, init_sigma2=1.5):\n",
    "    \"\"\"\n",
    "    Creates the quantities needed to construct the second order parameter expanded Taylor control variates for the log_density.\n",
    "    Output from this function will go into the function eval_q_k\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    p = len(thetaStar)\n",
    "\n",
    "    # Declare the output \n",
    "    dens_at_thetaStar = np.zeros(N)\n",
    "    grad_at_thetaStar = np.zeros([N, p])\n",
    "    Hess_at_thetaStar = np.zeros([N, p, p])\n",
    "\n",
    "    # Run the Kalman filter for gradient\n",
    "    analytical_grad_hess_result = analytical_grad_hess(thetaStar, init_sigma2, y=y)\n",
    "    dens_at_thetaStar = analytical_grad_hess_result[0]['log_likelihood_at_i']\n",
    "    grad_dens = analytical_grad_hess_result[0]['analytical_grad_at_i']\n",
    "\n",
    "    # Calculate gradient at thetaStar\n",
    "    grads_all = lambda param: np.stack(\n",
    "        analytical_grad_hess(param, init_sigma2=init_sigma2, y=y)[0]['analytical_grad_at_i'], axis=0\n",
    "    ) \n",
    "    Hess_all_at_thetaStar = jacobian(grads_all)(thetaStar)  # (N, p, p)\n",
    "\n",
    "    \n",
    "    for t in range(N):    \n",
    "        if t % 100 == 0:\n",
    "            print(\"Processed %s observations (out of % s)\" % (t, N))\n",
    "\n",
    "        grad_at_thetaStar[t, :], Hess_at_thetaStar[t, :] = grad_dens[t], Hess_all_at_thetaStar[t]\n",
    "                    \n",
    "    return np.array(dens_at_thetaStar), np.array(grad_at_thetaStar), np.array(Hess_at_thetaStar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafe25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dens_at_thetaStar_ARMA, grad_at_thetaStar_ARMA, Hess_at_thetaStar_ARMA = initiate_control_variate_quantities(thetaStar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_q_k(theta, dens_at_thetaStar, grad_at_thetaStar, Hess_at_thetaStar, order = 2):\n",
    "    \"\"\"\n",
    "    Evaluates the order order parameter expanded Taylor control variates at the point theta for all observations in \n",
    "    dens_at_thetaStar, grad_at_thetaStar, Hess_at_thetaStar. Default order is 2.\n",
    "    \"\"\"\n",
    "    const_term = dens_at_thetaStar\n",
    "    if order == 0:\n",
    "        q_k = const_term\n",
    "    elif order == 1:\n",
    "        first_term = np.sum(grad_at_thetaStar*(theta - thetaStar), axis = 1)\n",
    "        q_k = const_term + first_term\n",
    "    elif order == 2:\n",
    "        first_term = np.sum(grad_at_thetaStar*(theta - thetaStar), axis = 1)\n",
    "        second_term = 0.5*np.sum(np.sum(Hess_at_thetaStar*np.outer(theta - thetaStar, theta - thetaStar), axis = 1), axis = 1)    \n",
    "        q_k = const_term + first_term + second_term\n",
    "    else:\n",
    "        raise ValueError(\"Order must be 0 <= order <= 2\")\n",
    "    return q_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d198f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate gradient at thetaStar\u001b[39;00m\n\u001b[1;32m      6\u001b[0m grads_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m param: np\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m      7\u001b[0m     analytical_grad_hess(param, init_sigma2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, y\u001b[38;5;241m=\u001b[39my)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalytical_grad_at_i\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m ) \n\u001b[0;32m----> 9\u001b[0m Hess_all_at_MAP \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_all\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMAP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m obj_log_prior \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m theta: \u001b[38;5;241m-\u001b[39mlog_prior(theta)\n\u001b[1;32m     12\u001b[0m Hess_log_prior \u001b[38;5;241m=\u001b[39m hessian(obj_log_prior)\n",
      "File \u001b[0;32m~/Desktop/study/UTS/35112 Mathematical Research/winston-uts-research/venv/lib/python3.9/site-packages/autograd/wrap_util.py:23\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnum)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munary_operator\u001b[49m\u001b[43m(\u001b[49m\u001b[43munary_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnary_op_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/study/UTS/35112 Mathematical Research/winston-uts-research/venv/lib/python3.9/site-packages/autograd/differential_operators.py:71\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m jacobian_shape \u001b[38;5;241m=\u001b[39m ans_vspace\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m+\u001b[39m vspace(x)\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     70\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(vjp, ans_vspace\u001b[38;5;241m.\u001b[39mstandard_basis())\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mreshape(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m, jacobian_shape)\n",
      "File \u001b[0;32m~/Desktop/study/UTS/35112 Mathematical Research/winston-uts-research/venv/lib/python3.9/site-packages/autograd/numpy/numpy_wrapper.py:107\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstack\u001b[39m(arrays, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# this code is basically copied from numpy/core/shape_base.py's stack\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# we need it here because we want to re-implement stack in terms of the\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# primitives defined in this file\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [array(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/study/UTS/35112 Mathematical Research/winston-uts-research/venv/lib/python3.9/site-packages/autograd/numpy/numpy_wrapper.py:107\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstack\u001b[39m(arrays, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# this code is basically copied from numpy/core/shape_base.py's stack\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# we need it here because we want to re-implement stack in terms of the\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# primitives defined in this file\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [array(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/study/UTS/35112 Mathematical Research/winston-uts-research/venv/lib/python3.9/site-packages/autograd/core.py:20\u001b[0m, in \u001b[0;36mmake_vjp.<locals>.vjp\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvjp\u001b[39m(g):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_node\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/study/UTS/35112 Mathematical Research/winston-uts-research/venv/lib/python3.9/site-packages/autograd/core.py:31\u001b[0m, in \u001b[0;36mbackward_pass\u001b[0;34m(g, end_node)\u001b[0m\n\u001b[1;32m     29\u001b[0m     ingrads \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mvjp(outgrad[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m parent, ingrad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39mparents, ingrads):\n\u001b[0;32m---> 31\u001b[0m         outgrads[parent] \u001b[38;5;241m=\u001b[39m \u001b[43madd_outgrads\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutgrads\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mingrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outgrad[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/study/UTS/35112 Mathematical Research/winston-uts-research/venv/lib/python3.9/site-packages/autograd/core.py:187\u001b[0m, in \u001b[0;36madd_outgrads\u001b[0;34m(prev_g_flagged, g)\u001b[0m\n\u001b[1;32m    185\u001b[0m sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(g) \u001b[38;5;129;01min\u001b[39;00m sparse_object_types\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_g_flagged:\n\u001b[0;32m--> 187\u001b[0m     vs \u001b[38;5;241m=\u001b[39m \u001b[43mvspace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     prev_g, mutable \u001b[38;5;241m=\u001b[39m prev_g_flagged\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mutable:\n",
      "File \u001b[0;32m~/Desktop/study/UTS/35112 Mathematical Research/winston-uts-research/venv/lib/python3.9/site-packages/autograd/core.py:290\u001b[0m, in \u001b[0;36mvspace\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvspace\u001b[39m(value):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVSpace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmappings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m isbox(value):\n",
      "File \u001b[0;32m~/Desktop/study/UTS/35112 Mathematical Research/winston-uts-research/venv/lib/python3.9/site-packages/autograd/numpy/numpy_vspaces.py:69\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_covector\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconj(x)\n\u001b[0;32m---> 69\u001b[0m VSpace\u001b[38;5;241m.\u001b[39mregister(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28;01mlambda\u001b[39;00m x: ComplexArrayVSpace(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miscomplexobj\u001b[49m(x) \u001b[38;5;28;01melse\u001b[39;00m ArrayVSpace(x))\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m type_ \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mfloat\u001b[39m, np\u001b[38;5;241m.\u001b[39mlongdouble, np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat16]:\n\u001b[1;32m     72\u001b[0m     ArrayVSpace\u001b[38;5;241m.\u001b[39mregister(type_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dens_at_thetaStar, grad_at_thetaStar, Hess_at_thetaStar = dens_at_thetaStar_ARMA, grad_at_thetaStar_ARMA, Hess_at_thetaStar_ARMA\n",
    "\n",
    "d = len(theta0) \n",
    "\n",
    "# Calculate gradient at thetaStar\n",
    "grads_all = lambda param: np.stack(\n",
    "    analytical_grad_hess(param, init_sigma2=1.5, y=y)[0]['analytical_grad_at_i'], axis=0\n",
    ") \n",
    "Hess_all_at_MAP = jacobian(grads_all)(MAP)\n",
    "\n",
    "obj_log_prior = lambda theta: -log_prior(theta)\n",
    "Hess_log_prior = hessian(obj_log_prior)\n",
    "\n",
    "# Proposal covariance\n",
    "Sigma_pi = np.linalg.inv(-np.sum(Hess_all_at_MAP, axis = 0) + Hess_log_prior(MAP)) # Negative Hessian inverse evaluated at the mode\n",
    "PropCov = 2.38**2/d*Sigma_pi\n",
    "\n",
    "# Get ready for pseudo marginal Metropolis-Hastings sampling\n",
    "Taylor_order = 2\n",
    "N_sim = 22000 # MCMC samples\n",
    "G = 10 # Number of blocks\n",
    "m = 100 # Subsample size\n",
    "u_c = np.random.randint(0, N, m)\n",
    "u_block_indicators = np.hstack((np.repeat(np.arange(G-1), m/G), np.repeat(G-1, m - len(np.repeat(np.arange(G-1), m/G))))) # Splits the u into blocks\n",
    "\n",
    "# Precomputed quantities for the control variates that do not depend on theta\n",
    "# Following the notation in the slides\n",
    "A = np.sum(dens_at_thetaStar) \n",
    "B = np.sum(grad_at_thetaStar, axis = 0)\n",
    "C = np.sum(Hess_at_thetaStar, axis = 0)\n",
    "\n",
    "if Taylor_order == 0:\n",
    "    q_sum = lambda theta: A\n",
    "elif Taylor_order == 1:\n",
    "    q_sum = lambda theta: A + np.dot(B, theta - thetaStar)\n",
    "elif Taylor_order == 2:\n",
    "    q_sum = lambda theta: A + np.dot(B, theta - thetaStar) + 0.5*np.dot(theta - thetaStar, np.dot(C, theta - thetaStar))\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Storage\n",
    "theta_init = MAP + sps.norm.rvs(0, 0.01, len(MAP))\n",
    "samples = np.zeros((N_sim + 1, d))\n",
    "samples[0, :] = theta_init\n",
    "alphas = np.zeros(N_sim)\n",
    "log_posthat_samples = np.zeros(N_sim + 1) # Keeps the estimated value of the log-posterior (up to a normalisation constant)\n",
    "sigma2_lhat_samples = np.zeros(N_sim + 1) # Keeps the estimated variance of the log-likelihood estimator \n",
    "\n",
    "# Current parameter and evaluate quantities\n",
    "theta_c = theta_init   \n",
    "l_k_c = analytical_grad_hess(theta_c, init_sigma2=1.5,y=y[u_c])[2]  # log-densities at current theta, u\n",
    "q_k_c = eval_q_k(theta_c, dens_at_thetaStar[u_c], grad_at_thetaStar[u_c], Hess_at_thetaStar[u_c], order = Taylor_order)  # control variates at current theta, u\n",
    "log_posthat_c = q_sum(theta_c) + N*np.mean(l_k_c - q_k_c) + log_prior(theta_c)\n",
    "sigma2_lhat_c = N**2/m*np.var(l_k_c - q_k_c, ddof = 1)\n",
    "log_posthat_samples[0] = log_posthat_c\n",
    "sigma2_lhat_samples[0] = sigma2_lhat_c\n",
    "\n",
    "# Check\n",
    "print('Estimate')\n",
    "print(q_sum(theta_c) +  N*np.mean(l_k_c - q_k_c))\n",
    "print('True value')\n",
    "print(analytical_grad_hess(theta_c, init_sigma2=1.5,y=y)[2])\n",
    "\n",
    "tic = time.time()\n",
    "print('Subsampling MCMC for the Poisson regression model with n = %s, m = %s, G = %s and Taylor order = %s' % (N, m, G, Taylor_order))\n",
    "for i in range(1, N_sim + 1):\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Iteration i = {}. Acceptance prob (mean) {:.2f}. Time: {:.2f}\".format(i , np.mean(alphas[:i]), time.time() - tic))\n",
    "\n",
    "    # Propose parameter vector and subsample\n",
    "    theta_p = np.random.multivariate_normal(theta_c, PropCov, size = 1).flatten() # flatten() to get 1-dim array \n",
    "    \n",
    "    block_to_update = np.random.randint(0, G, 1)[0]\n",
    "    u_p = copy.copy(u_c)\n",
    "    indices_to_update = (u_block_indicators == block_to_update)\n",
    "    u_p[indices_to_update] = np.random.randint(0, N, np.sum(indices_to_update)) \n",
    "    \n",
    "    # TODO: \n",
    "    # 1. Evaluate log densities and control variates at theta_p, u_p. \n",
    "    # 2. Estimate the log-likelihood and the variance of the log-likelihood\n",
    "    l_k_p = analytical_grad_hess(theta_p, init_sigma2=1.5,y=y[u_p])[2] # '?' # Your code instead of '?' # log-densities at proposed theta, u\n",
    "    q_k_p = eval_q_k(theta_p, dens_at_thetaStar[u_p], grad_at_thetaStar[u_p], Hess_at_thetaStar[u_p], order = Taylor_order) # '?' # Your code instead of '?'  # control variates at proposed theta, u\n",
    "    log_posthat_p = q_sum(theta_p) + N*np.mean(l_k_p - q_k_p) + log_prior(theta_p) # '?' # Your code instead of '?' # log-likelihood estimator at proposed theta, u\n",
    "    sigma2_lhat_p = N**2/m*np.var(l_k_p - q_k_p, ddof = 1) # '?' # Your code instead of '?' # estimate of variance of log-likelihood estimator at proposed theta, u\n",
    "    \n",
    "    # log proposal densities\n",
    "    log_q_p = sps.multivariate_normal.logpdf(theta_p, mean = theta_c, cov = PropCov) # log-proposal density. Symmetric (cancels), but leave for completeness           \n",
    "    log_q_c = sps.multivariate_normal.logpdf(theta_c, mean = theta_p, cov = PropCov)   \n",
    "    \n",
    "    alpha = np.min([1, np.exp(log_posthat_p - sigma2_lhat_p/2 - log_q_p - (log_posthat_c - sigma2_lhat_c/2 - log_q_c))])\n",
    "    alphas[i - 1] = alpha\n",
    "    if np.random.rand() < alpha: # sample Unif(0, 1) to determine acceptance\n",
    "        samples[i, :] = theta_p\n",
    "        log_posthat_samples[i] = log_posthat_p \n",
    "        sigma2_lhat_samples[i] = sigma2_lhat_p\n",
    "        # Proposed becomes current in next iteration\n",
    "        theta_c, log_posthat_c, sigma2_lhat_c, log_q_c, u_c = theta_p, log_posthat_p, sigma2_lhat_p, log_q_p, u_p\n",
    "    else:\n",
    "        samples[i, :] = theta_c \n",
    "        log_posthat_samples[i] = log_posthat_c \n",
    "        sigma2_lhat_samples[i] = sigma2_lhat_c\n",
    "\n",
    "\n",
    "matplotlib.rc('text', usetex = True)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(log_posthat_samples)\n",
    "ax.set(xlabel='MCMC iteration', ylabel= r'Estimated $log p(y|\\theta)p(\\theta)$' , \n",
    "       title= 'Estimated log-posterior values over MCMC iterations')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(sigma2_lhat_samples, color = 'red')\n",
    "ax.set(xlabel='MCMC iteration', ylabel= r'Estimated $\\sigma^2_{\\widehat{d}}(\\theta)$' , \n",
    "       title= 'Estimated variance of log-likelihood estimator over MCMC iterations')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Use 2000 as burn-in\n",
    "samples = samples[2000:] \n",
    "\n",
    "# Effective sample size and inefficiency factors\n",
    "ESS = np.zeros(samples.shape[1])\n",
    "for j in range(samples.shape[1]):\n",
    "    ESS[j] = effective_sample_size(samples[:, j])    \n",
    "IF = samples.shape[0]/ESS\n",
    "   \n",
    "print(\"Mean ESS: {:.2f}. Median ESS: {:.2f}. Min ESS: {:.2f}. Max ESS: {:.2f}\".format(np.mean(ESS), np.median(ESS), np.min(ESS), np.max(ESS)))\n",
    "print(\"Mean IF : {:.2f}. Median IF : {:.2f}. Min IF : {:.2f}. Max IF : {:.2f}\".format(np.mean(IF), np.median(IF), np.min(IF), np.max(IF)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
